print("--- SCRIPT ƒêANG KH·ªûI ƒê·ªòNG... ---")

import pandas as pd
import numpy as np
import os
import glob
import joblib
import re
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.metrics import mean_squared_error
import warnings

warnings.filterwarnings("ignore")

# ================= C·∫§U H√åNH (S·ª¨A ·ªû ƒê√ÇY) =================
SCORE_FOLDER = r'diem_thi_thptqg'  # <--- ƒê∆∞·ªùng d·∫´n folder ƒëi·ªÉm thi
BENCHMARK_FILE = 'diem_chuan_cleaned.csv'     # File ƒëi·ªÉm chu·∫©n (c√≥ nhi·ªÅu data v√† c·ªôt he_dao_tao)

# C·∫§U H√åNH TH∆Ø M·ª§C OUTPUT M·ªöI
OUTPUT_DIR = 'model_artifacts'  # T√™n th∆∞ m·ª•c b·∫°n mu·ªën ch·ª©a file .pkl

# T·ª± ƒë·ªông n·ªëi ƒë∆∞·ªùng d·∫´n (Kh√¥ng c·∫ßn s·ª≠a 2 d√≤ng d∆∞·ªõi)
MODEL_OUTPUT = os.path.join(OUTPUT_DIR, 'university_ranking_model_2026.pkl')
LOOKUP_OUTPUT = os.path.join(OUTPUT_DIR, 'score_distribution_2025.pkl')

# ƒê·ªãnh nghƒ©a T·∫§T C·∫¢ c√°c t·ªï h·ª£p m√¥n ph·ªï bi·∫øn
BLOCK_MAP = {
    'A00': ['toan', 'vat_ly', 'hoa_hoc'],
    'A01': ['toan', 'vat_ly', 'ngoai_ngu'],
    'A02': ['toan', 'vat_ly', 'sinh_hoc'],
    'B00': ['toan', 'hoa_hoc', 'sinh_hoc'],
    'B08': ['toan', 'sinh_hoc', 'ngoai_ngu'],
    'C00': ['ngu_van', 'lich_su', 'dia_ly'],
    'C03': ['ngu_van', 'toan', 'lich_su'],
    'C04': ['ngu_van', 'toan', 'dia_ly'],
    'D01': ['toan', 'ngu_van', 'ngoai_ngu'],
    'D07': ['toan', 'hoa_hoc', 'ngoai_ngu'],
    'D08': ['toan', 'sinh_hoc', 'ngoai_ngu'],  # Gi·ªëng B08 nh∆∞ng t√™n kh√°c
    'D13': ['toan', 'ngu_van', 'sinh_hoc'],
    'D66': ['ngu_van', 'gdcd', 'ngoai_ngu'],
}

# ================= 1. H√ÄM X·ª¨ L√ù ƒêI·ªÇM THI =================
def build_percentile_lookup(score_folder):
    print(f"\nüöÄ B∆Ø·ªöC 1: X√¢y d·ª±ng ph√¢n ph·ªëi ƒëi·ªÉm thi t·ª´: {score_folder}")
    lookup_dict = {}
    files = glob.glob(os.path.join(score_folder, "*.csv"))
    
    if not files:
        print(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file .csv n√†o trong {score_folder}")
        return {}

    print(f"   -> T√¨m th·∫•y {len(files)} file ƒëi·ªÉm thi.")

    for file_path in files:
        filename = os.path.basename(file_path)
        try:
            year = int(re.search(r'\d{4}', filename).group())
        except:
            continue
            
        print(f"   -> ƒêang x·ª≠ l√Ω file {filename}...")
        print(f"   -> ƒêang x·ª≠ l√Ω nƒÉm {year}...")
        all_cols = list(set([col for cols in BLOCK_MAP.values() for col in cols]))
        
        try:
            chunks = pd.read_csv(file_path, usecols=all_cols, chunksize=200000)
        except ValueError:
            continue

        block_data = {k: [] for k in BLOCK_MAP.keys()}
        
        for chunk in chunks:
            for block, cols in BLOCK_MAP.items():
                if not all(col in chunk.columns for col in cols): continue
                temp = chunk.dropna(subset=cols)
                if not temp.empty:
                    scores = temp[cols].sum(axis=1).tolist()
                    block_data[block].extend(scores)
        
        for block, scores in block_data.items():
            if not scores: continue
            scores_np = np.array(scores)
            scores_np.sort()
            scores_np = scores_np[::-1]
            
            df_score = pd.DataFrame({'score': scores_np})
            df_score['rank'] = df_score['score'].rank(method='min', ascending=False)
            total = len(df_score)
            
            lookup = df_score.groupby('score')['rank'].min().reset_index()
            lookup['percentile'] = (lookup['rank'] / total) * 100
            lookup_dict[(year, block)] = lookup.sort_values('score')

    print(f"\n‚úÖ Ho√†n th√†nh B∆∞·ªõc 1. ƒê√£ x·ª≠ l√Ω {len(lookup_dict)} c·∫∑p (NƒÉm, T·ªï h·ª£p).")
    return lookup_dict

# ================= 2. H√ÄM QUY ƒê·ªîI ƒêI·ªÇM CHU·∫®N =================
def expand_multiple_blocks(df):
    """
    T√°ch c√°c d√≤ng c√≥ nhi·ªÅu t·ªï h·ª£p (vd: 'A00, B00') th√†nh nhi·ªÅu d√≤ng ri√™ng
    """
    expanded_rows = []
    
    for _, row in df.iterrows():
        to_hop = str(row['to_hop_mon']).strip()
        
        # T√°ch n·∫øu c√≥ nhi·ªÅu t·ªï h·ª£p (ph√¢n c√°ch b·∫±ng d·∫•u ph·∫©y)
        blocks = [b.strip() for b in to_hop.split(',')]
        
        for block in blocks:
            if block and block in BLOCK_MAP:  # Ch·ªâ l·∫•y t·ªï h·ª£p ƒë√£ ƒë·ªãnh nghƒ©a
                new_row = row.copy()
                new_row['to_hop_mon'] = block
                expanded_rows.append(new_row)
    
    return pd.DataFrame(expanded_rows)

def normalize_benchmark(benchmark_file, lookup_dict):
    print(f"\nüöÄ B∆Ø·ªöC 2: Quy ƒë·ªïi ƒëi·ªÉm chu·∫©n t·ª´ {benchmark_file}...")
    try:
        df = pd.read_csv(benchmark_file)
    except:
        return pd.DataFrame()
    
    # T√°ch c√°c d√≤ng c√≥ nhi·ªÅu t·ªï h·ª£p
    print(f"   -> ƒêang t√°ch c√°c d√≤ng c√≥ nhi·ªÅu t·ªï h·ª£p...")
    df_expanded = expand_multiple_blocks(df)
    print(f"   -> S·ªë d√≤ng sau khi t√°ch: {len(df_expanded)} (t·ª´ {len(df)} d√≤ng g·ªëc)")
    
    # L·ªåC CH·ªà L·∫§Y NG√ÄNH C√ì D·ªÆ LI·ªÜU NƒÇM 2025
    print(f"   -> ƒêang l·ªçc c√°c ng√†nh c√≥ d·ªØ li·ªáu nƒÉm 2025...")
    nganh_co_2025 = df_expanded[df_expanded['nam'] == 2025][['university_id', 'ma_nganh', 'to_hop_mon']].drop_duplicates()
    print(f"   -> T√¨m th·∫•y {len(nganh_co_2025)} ng√†nh c√≥ d·ªØ li·ªáu 2025")
    
    # Merge ƒë·ªÉ ch·ªâ gi·ªØ l·∫°i c√°c d√≤ng thu·ªôc ng√†nh c√≥ nƒÉm 2025
    df_filtered = df_expanded.merge(
        nganh_co_2025, 
        on=['university_id', 'ma_nganh', 'to_hop_mon'], 
        how='inner'
    )
    print(f"   -> S·ªë d√≤ng sau khi l·ªçc: {len(df_filtered)}")
    
    def get_percentile(row):
        year = row['nam']
        block = str(row['to_hop_mon']).strip()
        score = row['diem_chuan']
        lookup = lookup_dict.get((year, block))
        if lookup is None: return np.nan 
        idx = np.searchsorted(lookup['score'], score, side='left')
        if idx < len(lookup): return lookup.iloc[idx]['percentile']
        else: return 0.01

    df_filtered['percentile_rank'] = df_filtered.apply(get_percentile, axis=1)
    df_clean = df_filtered.dropna(subset=['percentile_rank'])
    print(f"‚úÖ Ho√†n th√†nh B∆∞·ªõc 2. D·ªØ li·ªáu hu·∫•n luy·ªán: {len(df_clean)} d√≤ng.")
    return df_clean

# ================= 3. MODEL ENGINE =================
def predict_weighted_average(values):
    n = len(values)
    if n == 0: return 0
    weights = np.arange(1, n + 1)
    return np.sum(values * weights) / weights.sum()

def predict_ets(values):
    try:
        if len(values) < 4: return None
        model = ExponentialSmoothing(values, trend='add', seasonal=None, initialization_method="estimated")
        fit = model.fit()
        return fit.forecast(1)[0]
    except: return None

def train_and_forecast_smart(df_labeled):
    print("\nüöÄ B∆Ø·ªöC 3: ƒê·∫•u Model (Backtesting) & D·ª± b√°o 2026...")
    groups = df_labeled.groupby(['university_id', 'ma_nganh', 'to_hop_mon'])
    forecast_results = {}
    stats = {'WA': 0, 'ETS': 0}
    processed = 0
    total_groups = len(groups)

    for name, group in groups:
        processed += 1
        if processed % 100 == 0: print(f"   -> ƒê√£ x·ª≠ l√Ω {processed}/{total_groups} ng√†nh...", end="\r")

        group = group.sort_values('nam')
        percentiles = group['percentile_rank'].values
        
        if len(percentiles) < 4:
            pred = predict_weighted_average(percentiles)
            forecast_results[name] = pred * 0.95
            continue

        train = percentiles[:-1]
        test = percentiles[-1]
        
        pred_wa = predict_weighted_average(train)
        pred_ets = predict_ets(train)
        
        err_wa = abs(pred_wa - test)
        err_ets = abs(pred_ets - test) if pred_ets is not None else float('inf')
        
        final_pred = 0
        if pred_ets is not None and err_ets < err_wa:
            stats['ETS'] += 1
            full_ets = predict_ets(percentiles)
            final_pred = full_ets if full_ets is not None else predict_weighted_average(percentiles)
        else:
            stats['WA'] += 1
            final_pred = predict_weighted_average(percentiles)
            
        forecast_results[name] = final_pred * 0.95

    print(f"\n‚úÖ Ho√†n th√†nh. T·ªâ s·ªë: Weighted Avg th·∫Øng {stats['WA']} - ETS th·∫Øng {stats['ETS']}")
    return forecast_results

# ================= MAIN =================
if __name__ == "__main__":
    print("\n================================================")
    print(f"INPUT FOLDER: {SCORE_FOLDER}")
    print(f"OUTPUT DIR:   {OUTPUT_DIR}")
    print("================================================")
    
    if not os.path.exists(SCORE_FOLDER):
        print(f"‚ùå L·ªñI: Th∆∞ m·ª•c input '{SCORE_FOLDER}' kh√¥ng t·ªìn t·∫°i!")
        exit()

    # --- T·∫†O TH∆Ø M·ª§C OUTPUT N·∫æU CH∆ØA C√ì ---
    if not os.path.exists(OUTPUT_DIR):
        print(f"üìÅ ƒêang t·∫°o th∆∞ m·ª•c m·ªõi: {OUTPUT_DIR}")
        os.makedirs(OUTPUT_DIR)
    
    # 1. Build
    lookup_map = build_percentile_lookup(SCORE_FOLDER)
    if not lookup_map: exit()
        
    # 2. Normalize
    df_train = normalize_benchmark(BENCHMARK_FILE, lookup_map)
    if df_train.empty: exit()
        
    # 3. Train
    model_2026 = train_and_forecast_smart(df_train)
    
    # 4. Save
    print(f"\nüíæ ƒêang l∆∞u file k·∫øt qu·∫£ v√†o '{OUTPUT_DIR}'...")
    joblib.dump(model_2026, MODEL_OUTPUT)
    
    lookup_2025 = {k: v for k, v in lookup_map.items() if k[0] == 2025}
    joblib.dump(lookup_2025, LOOKUP_OUTPUT)
    
    print("\nüéâüéâüéâ TH√ÄNH C√îNG! KI·ªÇM TRA TH∆Ø M·ª§C:", os.path.abspath(OUTPUT_DIR))
    input("·∫§n Enter ƒë·ªÉ tho√°t...")