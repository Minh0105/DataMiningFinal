"""
ADMISSION PROBABILITY MODEL v4 - ADVANCED VERSION (11 FEATURES)
Train model dá»± Ä‘oÃ¡n xÃ¡c suáº¥t Ä‘áº­u vá»›i cÃ¡c features nÃ¢ng cao

Features (11 total):
1. student_percentile    - Vá»‹ trÃ­ thÃ­ sinh (Top X%)
2. percentile_required   - NgÃ nh yÃªu cáº§u Top bao nhiÃªu %
3. uni_enc               - Encoded trÆ°á»ng
4. nganh_enc             - Encoded ngÃ nh
5. block_enc             - Encoded tá»• há»£p
6. gap                   - Khoáº£ng cÃ¡ch (student - required)
7. relative_position     - Tá»· lá»‡ vá»‹ trÃ­ (student / required)
8. trend                 - Xu hÆ°á»›ng Ä‘iá»ƒm chuáº©n qua cÃ¡c nÄƒm
9. volatility            - Äá»™ biáº¿n Ä‘á»™ng Ä‘iá»ƒm chuáº©n
10. school_prestige      - Äá»™ khÃ³ trung bÃ¬nh cá»§a trÆ°á»ng
11. block_competition    - Sá»‘ thÃ­ sinh trong tá»• há»£p
"""

import sys
sys.stdout.reconfigure(encoding='utf-8')

import pandas as pd
import numpy as np
import os
import glob
import joblib
import re
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
import warnings
warnings.filterwarnings("ignore")

print("="*70)
print("   ADMISSION PROBABILITY MODEL v4 - ADVANCED (11 FEATURES)")
print("="*70)

# ================= CONFIG =================
SCORE_FOLDER = 'diem_thi_thptqg'
BENCHMARK_FILE = 'diem_chuan_cleaned.csv'
OUTPUT_DIR = 'model_artifacts'

BLOCK_MAP = {
    'A00': ['toan', 'vat_ly', 'hoa_hoc'],
    'A01': ['toan', 'vat_ly', 'ngoai_ngu'],
    'A02': ['toan', 'vat_ly', 'sinh_hoc'],
    'B00': ['toan', 'hoa_hoc', 'sinh_hoc'],
    'B08': ['toan', 'sinh_hoc', 'ngoai_ngu'],
    'D01': ['toan', 'ngu_van', 'ngoai_ngu'],
    'D07': ['toan', 'hoa_hoc', 'ngoai_ngu'],
    'D08': ['toan', 'sinh_hoc', 'ngoai_ngu'],
    'D13': ['toan', 'ngu_van', 'sinh_hoc'],
}

# ================= STEP 1: LOAD DATA =================
print("\n[STEP 1] Loading data...")

df_benchmark = pd.read_csv(BENCHMARK_FILE)
print(f"   ğŸ“Š Äiá»ƒm chuáº©n: {len(df_benchmark):,} records")

# Load táº¥t cáº£ Ä‘iá»ƒm thi Ä‘á»ƒ xÃ¢y dá»±ng percentile lookup
print("   ğŸ“ Loading Ä‘iá»ƒm thi THPT (2018-2025)...")

all_score_data = {}  # {(year, block): scores array}
files = glob.glob(os.path.join(SCORE_FOLDER, '*.csv'))

for f in sorted(files):
    year_match = re.search(r'\d{4}', os.path.basename(f))
    if not year_match:
        continue
    year = int(year_match.group())
    
    print(f"      NÄƒm {year}...", end=" ")
    df_scores = pd.read_csv(f)
    
    blocks_loaded = []
    for block, cols in BLOCK_MAP.items():
        if all(c in df_scores.columns for c in cols):
            temp = df_scores.dropna(subset=cols)
            scores = temp[cols].sum(axis=1).values
            all_score_data[(year, block)] = scores
            blocks_loaded.append(f"{block}({len(scores):,})")
    
    print(", ".join(blocks_loaded))

print(f"   âœ… Loaded {len(all_score_data)} (year, block) combinations")

# ================= STEP 2: TÃNH PERCENTILE CHO ÄIá»‚M CHUáº¨N =================
print("\n[STEP 2] TÃ­nh percentile cho Ä‘iá»ƒm chuáº©n...")

def get_percentile(score, year, block):
    """TÃ­nh percentile cá»§a Ä‘iá»ƒm trong phÃ¢n phá»‘i nÄƒm Ä‘Ã³"""
    key = (year, block)
    if key not in all_score_data:
        return None
    scores = all_score_data[key]
    return (scores <= score).sum() / len(scores) * 100

# ThÃªm cá»™t percentile_required cho má»—i ngÃ nh
df_benchmark['percentile_required'] = df_benchmark.apply(
    lambda row: get_percentile(row['diem_chuan'], row['nam'], row['to_hop_mon']), 
    axis=1
)
df_benchmark = df_benchmark.dropna(subset=['percentile_required'])
print(f"   âœ… {len(df_benchmark):,} records cÃ³ percentile")

# ================= STEP 2.5: TÃNH ADVANCED FEATURES =================
print("\n[STEP 2.5] TÃ­nh toÃ¡n Advanced Features...")

# a) School Prestige: Äá»™ khÃ³ trung bÃ¬nh cá»§a trÆ°á»ng (dá»±a trÃªn Ä‘iá»ƒm chuáº©n trung bÃ¬nh)
school_prestige = df_benchmark.groupby('university_id')['diem_chuan'].mean().to_dict()
print(f"   ğŸ“Š School Prestige: {len(school_prestige)} trÆ°á»ng")

# b) Block Competition: Sá»‘ lÆ°á»£ng thÃ­ sinh trong má»—i tá»• há»£p (Ä‘á»™ cáº¡nh tranh)
block_competition = {}
for key, scores in all_score_data.items():
    nam, block = key
    if block not in block_competition:
        block_competition[block] = 0
    block_competition[block] += len(scores)
print(f"   ğŸ“Š Block Competition: {len(block_competition)} tá»• há»£p")
for block, count in sorted(block_competition.items(), key=lambda x: -x[1]):
    print(f"      {block}: {count:,} thÃ­ sinh")

# c) Trend & Volatility: Xu hÆ°á»›ng vÃ  biáº¿n Ä‘á»™ng Ä‘iá»ƒm chuáº©n qua cÃ¡c nÄƒm
trend_data = {}
volatility_data = {}
for (uni, nganh, block), grp in df_benchmark.groupby(['university_id', 'ma_nganh', 'to_hop_mon']):
    sorted_grp = grp.sort_values('nam')
    dc_values = sorted_grp['diem_chuan'].values
    
    # Trend: há»‡ sá»‘ gÃ³c cá»§a Ä‘Æ°á»ng xu hÆ°á»›ng (positive = tÄƒng, negative = giáº£m)
    if len(dc_values) >= 2:
        years = np.arange(len(dc_values))
        slope, _ = np.polyfit(years, dc_values, 1)
        trend_data[(uni, nganh, block)] = slope
        volatility_data[(uni, nganh, block)] = np.std(dc_values)
    else:
        trend_data[(uni, nganh, block)] = 0
        volatility_data[(uni, nganh, block)] = 0

print(f"   ğŸ“Š Trend & Volatility: {len(trend_data)} nhÃ³m (uni, nganh, block)")

# Thá»‘ng kÃª trend
trends = list(trend_data.values())
print(f"      Trend range: {min(trends):.2f} Ä‘áº¿n {max(trends):.2f} (Ä‘iá»ƒm/nÄƒm)")
print(f"      Trend mean: {np.mean(trends):.2f}")

# ================= STEP 3: Táº O TRAINING DATA =================
print("\n[STEP 3] Táº¡o training data vá»›i 11 FEATURES...")

training_data = []
sample_per_major = 300

for _, row in df_benchmark.iterrows():
    block = str(row['to_hop_mon']).strip()
    nam = row['nam']
    key = (nam, block)
    
    if key not in all_score_data:
        continue
    
    scores = all_score_data[key]
    diem_chuan = row['diem_chuan']
    percentile_required = row['percentile_required']
    university_id = row['university_id']
    ma_nganh = row['ma_nganh']
    
    # Láº¥y cÃ¡c features bá»• sung
    group_key = (university_id, ma_nganh, block)
    trend = trend_data.get(group_key, 0)
    volatility = volatility_data.get(group_key, 0)
    prestige = school_prestige.get(university_id, 25)  # default = 25 Ä‘iá»ƒm
    competition = block_competition.get(block, 10000)   # default = 10000
    
    n_samples = min(sample_per_major, len(scores))
    sampled = np.random.choice(scores, size=n_samples, replace=False)
    
    for score in sampled:
        # Label: Ä‘áº­u náº¿u Ä‘iá»ƒm >= Ä‘iá»ƒm chuáº©n
        label = 1 if score >= diem_chuan else 0
        student_percentile = (scores <= score).sum() / len(scores) * 100
        
        # === ADVANCED FEATURES ===
        gap = student_percentile - percentile_required
        relative_position = student_percentile / (percentile_required + 0.01)  # +0.01 trÃ¡nh chia 0
        
        training_data.append({
            'university_id': university_id,
            'ma_nganh': ma_nganh,
            'to_hop_mon': block,
            'nam': nam,
            'diem_thi': score,
            # === Core Features ===
            'student_percentile': student_percentile,
            'percentile_required': percentile_required,
            # === NEW Advanced Features ===
            'gap': gap,
            'relative_position': relative_position,
            'trend': trend,
            'volatility': volatility,
            'school_prestige': prestige,
            'block_competition': competition,
            'label': label
        })

df_train = pd.DataFrame(training_data)
print(f"   âœ… Táº¡o Ä‘Æ°á»£c {len(df_train):,} training samples")
print(f"      Äáº­u: {(df_train['label']==1).sum():,} ({(df_train['label']==1).mean()*100:.1f}%)")
print(f"      TrÆ°á»£t: {(df_train['label']==0).sum():,} ({(df_train['label']==0).mean()*100:.1f}%)")

# ================= STEP 4: FEATURE ENGINEERING =================
print("\n[STEP 4] Feature Engineering (11 Features)...")

le_university = LabelEncoder()
le_nganh = LabelEncoder()
le_block = LabelEncoder()

df_train['uni_enc'] = le_university.fit_transform(df_train['university_id'].astype(str))
df_train['nganh_enc'] = le_nganh.fit_transform(df_train['ma_nganh'].astype(str))
df_train['block_enc'] = le_block.fit_transform(df_train['to_hop_mon'])

# 11 FEATURES - Ä‘áº§y Ä‘á»§ thÃ´ng tin Ä‘á»ƒ dá»± Ä‘oÃ¡n xÃ¡c suáº¥t Ä‘áº­u
feature_cols = [
    # === Core Features (5) ===
    'student_percentile',    # Vá»‹ trÃ­ cá»§a thÃ­ sinh (Top bao nhiÃªu %)
    'percentile_required',   # NgÃ nh yÃªu cáº§u Top bao nhiÃªu % (tá»« lá»‹ch sá»­)
    'uni_enc',               # Encoded trÆ°á»ng
    'nganh_enc',             # Encoded ngÃ nh
    'block_enc',             # Encoded tá»• há»£p
    # === NEW Advanced Features (6) ===
    'gap',                   # Khoáº£ng cÃ¡ch percentile (student - required)
    'relative_position',     # Tá»· lá»‡ vá»‹ trÃ­ (student / required)
    'trend',                 # Xu hÆ°á»›ng Ä‘iá»ƒm chuáº©n qua cÃ¡c nÄƒm
    'volatility',            # Äá»™ biáº¿n Ä‘á»™ng Ä‘iá»ƒm chuáº©n
    'school_prestige',       # Äá»™ khÃ³ trung bÃ¬nh cá»§a trÆ°á»ng
    'block_competition',     # Sá»‘ thÃ­ sinh trong tá»• há»£p
]

X = df_train[feature_cols].values
y = df_train['label'].values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print(f"   âœ… Features ({len(feature_cols)} total):")
for i, col in enumerate(feature_cols):
    print(f"      {i+1}. {col}")
print(f"   âœ… X shape: {X_scaled.shape}")

# ================= STEP 5: TRAIN MODELS =================
print("\n[STEP 5] Training Gradient Boosting...")

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)
print(f"   Train: {len(X_train):,}, Test: {len(X_test):,}")

# Train Gradient Boosting
gb_model = GradientBoostingClassifier(
    n_estimators=150,    # TÄƒng tá»« 100 lÃªn 150
    max_depth=6,         # TÄƒng tá»« 5 lÃªn 6 (vÃ¬ cÃ³ nhiá»u features hÆ¡n)
    learning_rate=0.1,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42
)
gb_model.fit(X_train, y_train)

y_pred = gb_model.predict(X_test)
y_prob = gb_model.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)

print(f"\n   ğŸ“Š RESULTS:")
print(f"      Accuracy: {accuracy:.4f}")
print(f"      ROC-AUC: {roc_auc:.4f}")

# Cross validation
cv_scores = cross_val_score(gb_model, X_scaled, y, cv=5, scoring='roc_auc')
print(f"      CV Score: {cv_scores.mean():.4f} (Â±{cv_scores.std()*2:.4f})")

# Feature importance
print(f"\n   ğŸ“ˆ Feature Importance:")
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': gb_model.feature_importances_
}).sort_values('importance', ascending=False)

for _, row in importance_df.iterrows():
    bar = 'â–ˆ' * int(row['importance'] * 50)
    print(f"      {row['feature']:20s}: {row['importance']:.4f} {bar}")

# ================= STEP 6: TÃNH PERCENTILE YÃŠU Cáº¦U TRUNG BÃŒNH =================
print("\n[STEP 6] TÃ­nh percentile trung bÃ¬nh cho má»—i ngÃ nh...")

avg_percentile = df_benchmark.groupby(['university_id', 'ma_nganh', 'to_hop_mon']).agg({
    'percentile_required': 'mean',
    'diem_chuan': 'mean'
}).reset_index()
avg_percentile.columns = ['university_id', 'ma_nganh', 'to_hop_mon', 'avg_percentile_required', 'avg_diem_chuan']

print(f"   âœ… {len(avg_percentile)} ngÃ nh")

# ================= STEP 7: SAVE MODELS =================
print("\n[STEP 7] Saving models...")

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# Save model
joblib.dump(gb_model, os.path.join(OUTPUT_DIR, 'admission_probability_model.pkl'))
print(f"   âœ… Saved: admission_probability_model.pkl")

# Save encoders vá»›i feature_cols
encoders = {
    'university': le_university,
    'nganh': le_nganh,
    'block': le_block,
    'feature_cols': feature_cols  # QUAN TRá»ŒNG: Ä‘á»ƒ web_app biáº¿t dÃ¹ng bao nhiÃªu features
}
joblib.dump(encoders, os.path.join(OUTPUT_DIR, 'admission_encoders.pkl'))
print(f"   âœ… Saved: admission_encoders.pkl")

# Save scaler
joblib.dump(scaler, os.path.join(OUTPUT_DIR, 'admission_scaler.pkl'))
print(f"   âœ… Saved: admission_scaler.pkl")

# Save avg_percentile lookup
joblib.dump(avg_percentile, os.path.join(OUTPUT_DIR, 'major_percentile_lookup.pkl'))
print(f"   âœ… Saved: major_percentile_lookup.pkl")

# Save score data 2025 Ä‘á»ƒ dÃ¹ng cho web app
score_data_2025 = {k: v for k, v in all_score_data.items() if k[0] == 2025}
joblib.dump(score_data_2025, os.path.join(OUTPUT_DIR, 'score_data_2025.pkl'))
print(f"   âœ… Saved: score_data_2025.pkl")

# *** Má»šI: Save advanced features metadata ***
advanced_features = {
    'school_prestige': school_prestige,      # dict: university_id -> avg_diem_chuan
    'block_competition': block_competition,  # dict: block -> num_students
    'trend': trend_data,                     # dict: (uni, nganh, block) -> slope
    'volatility': volatility_data            # dict: (uni, nganh, block) -> std
}
joblib.dump(advanced_features, os.path.join(OUTPUT_DIR, 'advanced_features.pkl'))
print(f"   âœ… Saved: advanced_features.pkl (NEW - trend, volatility, prestige, competition)")

# ================= STEP 8: DEMO =================
print("\n" + "="*70)
print("   DEMO: Dá»° ÄOÃN XÃC SUáº¤T Äáº¬U 2026 (11 Features)")
print("="*70)

def predict_admission_probability_v4(diem, block, university_id, ma_nganh):
    """Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t Ä‘áº­u cho nÄƒm 2026 vá»›i 11 features"""
    
    # 1. TÃ­nh percentile cá»§a thÃ­ sinh (dÃ¹ng phÃ¢n phá»‘i 2025)
    key = (2025, block)
    if key not in all_score_data:
        return None, None, None
    
    scores = all_score_data[key]
    student_percentile = (scores <= diem).sum() / len(scores) * 100
    
    # 2. Láº¥y percentile yÃªu cáº§u trung bÃ¬nh cá»§a ngÃ nh
    row = avg_percentile[
        (avg_percentile['university_id'] == university_id) & 
        (avg_percentile['ma_nganh'] == ma_nganh) &
        (avg_percentile['to_hop_mon'] == block)
    ]
    
    if row.empty:
        return None, None, None
    
    percentile_required = row.iloc[0]['avg_percentile_required']
    
    # 3. Encode features
    try:
        uni_enc = le_university.transform([str(university_id)])[0]
    except:
        uni_enc = 0
    try:
        nganh_enc = le_nganh.transform([str(ma_nganh)])[0]
    except:
        nganh_enc = 0
    try:
        block_enc = le_block.transform([block])[0]
    except:
        block_enc = 0
    
    # 4. TÃ­nh advanced features
    gap = student_percentile - percentile_required
    relative_position = student_percentile / (percentile_required + 0.01)
    
    group_key = (university_id, ma_nganh, block)
    trend = trend_data.get(group_key, 0)
    volatility = volatility_data.get(group_key, 0)
    prestige = school_prestige.get(university_id, 25)
    competition = block_competition.get(block, 10000)
    
    # 5. Predict vá»›i 11 features
    features = np.array([[
        student_percentile, percentile_required, uni_enc, nganh_enc, block_enc,
        gap, relative_position, trend, volatility, prestige, competition
    ]])
    features_scaled = scaler.transform(features)
    prob = gb_model.predict_proba(features_scaled)[0][1]
    
    return prob, student_percentile, percentile_required

# Test cases
test_cases = [
    (27.5, 'B00', 215, '7720101', 'Y khoa Huáº¿ - Äiá»ƒm ráº¥t cao (27.5)'),
    (26.0, 'B00', 215, '7720101', 'Y khoa Huáº¿ - Äiá»ƒm cao (26.0)'),
    (24.5, 'B00', 215, '7720101', 'Y khoa Huáº¿ - Äiá»ƒm TB (24.5)'),
    (23.0, 'B00', 215, '7720101', 'Y khoa Huáº¿ - Äiá»ƒm tháº¥p (23.0)'),
    (26.0, 'B00', 215, '7720201', 'RÄƒng HÃ m Máº·t Huáº¿ (26.0)'),
    (24.0, 'B00', 215, '7720301', 'DÆ°á»£c há»c Huáº¿ (24.0)'),
]

for diem, block, uni, nganh, desc in test_cases:
    result = predict_admission_probability_v4(diem, block, uni, nganh)
    if result[0] is not None:
        prob, student_pct, required_pct = result
        print(f"\nğŸ“ {desc}")
        print(f"   Tá»• há»£p: {block} | Äiá»ƒm: {diem}")
        print(f"   Báº¡n: Top {100-student_pct:.2f}% | YÃªu cáº§u: Top {100-required_pct:.2f}%")
        
        # ThÃªm thÃ´ng tin vá» cÃ¡c features bá»• sung
        group_key = (uni, nganh, block)
        trend_val = trend_data.get(group_key, 0)
        vol_val = volatility_data.get(group_key, 0)
        
        trend_emoji = "ğŸ“ˆ" if trend_val > 0 else "ğŸ“‰" if trend_val < 0 else "â¡ï¸"
        print(f"   Trend: {trend_emoji} {trend_val:+.2f} Ä‘iá»ƒm/nÄƒm | Biáº¿n Ä‘á»™ng: {vol_val:.2f}")
        
        if prob >= 0.7:
            print(f"   >>> ğŸŸ¢ XÃC SUáº¤T Äáº¬U: {prob*100:.1f}% (Cao)")
        elif prob >= 0.4:
            print(f"   >>> ğŸŸ¡ XÃC SUáº¤T Äáº¬U: {prob*100:.1f}% (Trung bÃ¬nh)")
        else:
            print(f"   >>> ğŸ”´ XÃC SUáº¤T Äáº¬U: {prob*100:.1f}% (Tháº¥p)")
    else:
        print(f"\nâŒ {desc}: KhÃ´ng cÃ³ dá»¯ liá»‡u")

print("\n" + "="*70)
print("   âœ… HOÃ€N THÃ€NH - Model v4 vá»›i 11 Features!")
print("="*70)
print("\nğŸ“ Files Ä‘Ã£ lÆ°u:")
print("   - admission_probability_model.pkl (Gradient Boosting)")
print("   - admission_encoders.pkl (LabelEncoders + feature_cols)")
print("   - admission_scaler.pkl (StandardScaler)")
print("   - major_percentile_lookup.pkl (Lookup table)")
print("   - score_data_2025.pkl (PhÃ¢n phá»‘i Ä‘iá»ƒm 2025)")
print("   - advanced_features.pkl (trend, volatility, prestige, competition)")
