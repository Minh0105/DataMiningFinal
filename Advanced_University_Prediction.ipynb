{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59dfa56f",
   "metadata": {},
   "source": [
    "# üéì ADVANCED UNIVERSITY ADMISSION PREDICTION SYSTEM\n",
    "## H·ªá th·ªëng D·ª± b√°o ƒêi·ªÉm chu·∫©n ƒê·∫°i h·ªçc Y D∆∞·ª£c 2026\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Features n√¢ng cao:\n",
    "- **5 Models**: Weighted Average, ETS, ARIMA, Linear Regression, Ridge\n",
    "- **Ensemble Learning**: K·∫øt h·ª£p nhi·ªÅu model\n",
    "- **Cross-Validation**: Time Series CV\n",
    "- **Confidence Interval**: Kho·∫£ng tin c·∫≠y 95%\n",
    "- **Feature Engineering**: Trend, Volatility, Momentum\n",
    "- **Advanced Visualization**: Interactive charts\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: AI University Admission Predictor V2.0  \n",
    "**Data**: ƒêi·ªÉm chu·∫©n Y D∆∞·ª£c 2018-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f94c90",
   "metadata": {},
   "source": [
    "## üì¶ 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install statsmodels plotly kaleido scikit-learn pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315051f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# ML & Stats\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Colab\n",
    "from google.colab import files\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"‚úÖ Import th√†nh c√¥ng!\")\n",
    "print(f\"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba85f8",
   "metadata": {},
   "source": [
    "## üì§ 2. Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p diem_thi_thptqg\n",
    "\n",
    "print(\"üì§ Upload file diem_chuan_cleaned.csv\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe5504",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì§ Upload c√°c file ƒëi·ªÉm thi THPT (2018-2025)\")\n",
    "uploaded_scores = files.upload()\n",
    "\n",
    "for filename in uploaded_scores.keys():\n",
    "    if 'diem_thi' in filename:\n",
    "        !mv \"{filename}\" diem_thi_thptqg/\n",
    "        print(f\"  ‚úÖ {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a84eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_benchmark = pd.read_csv('diem_chuan_cleaned.csv')\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"üìä TH√îNG TIN D·ªÆ LI·ªÜU\")\n",
    "print(\"=\"*50)\n",
    "print(f\"S·ªë d√≤ng: {len(df_benchmark):,}\")\n",
    "print(f\"S·ªë c·ªôt: {len(df_benchmark.columns)}\")\n",
    "print(f\"C√°c nƒÉm: {sorted(df_benchmark['nam'].unique())}\")\n",
    "print(f\"S·ªë tr∆∞·ªùng: {df_benchmark['university_id'].nunique()}\")\n",
    "print(f\"S·ªë ng√†nh: {df_benchmark['ma_nganh'].nunique()}\")\n",
    "\n",
    "df_benchmark.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb47b4",
   "metadata": {},
   "source": [
    "## üìä 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Overview Statistics\n",
    "print(\"üìà TH·ªêNG K√ä T·ªîNG QUAN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "yearly_stats = df_benchmark.groupby('nam').agg({\n",
    "    'diem_chuan': ['mean', 'std', 'min', 'max', 'count'],\n",
    "    'university_id': 'nunique',\n",
    "    'ma_nganh': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "yearly_stats.columns = ['ƒêi·ªÉm TB', 'Std', 'Min', 'Max', 'S·ªë d√≤ng', 'S·ªë tr∆∞·ªùng', 'S·ªë ng√†nh']\n",
    "yearly_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Interactive Box Plot - ƒêi·ªÉm theo nƒÉm\n",
    "fig = px.box(df_benchmark, x='nam', y='diem_chuan',\n",
    "             color='nam',\n",
    "             title='üìä Ph√¢n b·ªë ƒêi·ªÉm chu·∫©n theo NƒÉm (2018-2025)',\n",
    "             labels={'nam': 'NƒÉm', 'diem_chuan': 'ƒêi·ªÉm chu·∫©n'},\n",
    "             color_discrete_sequence=px.colors.sequential.Viridis)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    showlegend=False,\n",
    "    xaxis_title=\"NƒÉm\",\n",
    "    yaxis_title=\"ƒêi·ªÉm chu·∫©n\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947bb086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Top 10 tr∆∞·ªùng ƒëi·ªÉm cao nh·∫•t 2025\n",
    "df_2025 = df_benchmark[df_benchmark['nam'] == 2025]\n",
    "top_schools = df_2025.groupby('ten_truong')['diem_chuan'].max().sort_values(ascending=True).tail(10)\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=top_schools.values,\n",
    "    y=top_schools.index,\n",
    "    orientation='h',\n",
    "    marker=dict(color=top_schools.values, colorscale='Reds')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='üèÜ Top 10 Tr∆∞·ªùng c√≥ ƒêi·ªÉm chu·∫©n cao nh·∫•t (2025)',\n",
    "    xaxis_title='ƒêi·ªÉm chu·∫©n',\n",
    "    yaxis_title='',\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d9365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Xu h∆∞·ªõng ƒëi·ªÉm chu·∫©n theo th·ªùi gian - Top 5 ng√†nh\n",
    "top_nganh = df_benchmark.groupby('ten_nganh').size().sort_values(ascending=False).head(5).index.tolist()\n",
    "df_top = df_benchmark[df_benchmark['ten_nganh'].isin(top_nganh)]\n",
    "trend = df_top.groupby(['nam', 'ten_nganh'])['diem_chuan'].mean().reset_index()\n",
    "\n",
    "fig = px.line(trend, x='nam', y='diem_chuan', color='ten_nganh',\n",
    "              title='üìà Xu h∆∞·ªõng ƒêi·ªÉm chu·∫©n Top 5 Ng√†nh (2018-2025)',\n",
    "              labels={'nam': 'NƒÉm', 'diem_chuan': 'ƒêi·ªÉm chu·∫©n TB', 'ten_nganh': 'Ng√†nh'},\n",
    "              markers=True)\n",
    "\n",
    "fig.update_layout(height=500, legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.3))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Heatmap - ƒêi·ªÉm chu·∫©n theo Tr∆∞·ªùng x NƒÉm\n",
    "pivot = df_benchmark.pivot_table(values='diem_chuan', index='ten_truong', columns='nam', aggfunc='max')\n",
    "pivot = pivot.loc[pivot.max(axis=1).sort_values(ascending=False).head(12).index]\n",
    "\n",
    "fig = px.imshow(pivot,\n",
    "                title='üî• Heatmap ƒêi·ªÉm chu·∫©n Max theo Tr∆∞·ªùng x NƒÉm',\n",
    "                labels=dict(x=\"NƒÉm\", y=\"Tr∆∞·ªùng\", color=\"ƒêi·ªÉm\"),\n",
    "                color_continuous_scale='RdYlGn',\n",
    "                aspect='auto')\n",
    "\n",
    "fig.update_layout(height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f145859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Violin Plot - Ph√¢n b·ªë ƒëi·ªÉm theo t·ªï h·ª£p\n",
    "top_blocks = df_benchmark['to_hop_mon'].value_counts().head(8).index.tolist()\n",
    "df_blocks = df_benchmark[df_benchmark['to_hop_mon'].isin(top_blocks)]\n",
    "\n",
    "fig = px.violin(df_blocks, x='to_hop_mon', y='diem_chuan', color='to_hop_mon',\n",
    "                title='üéª Ph√¢n b·ªë ƒêi·ªÉm chu·∫©n theo T·ªï h·ª£p m√¥n',\n",
    "                labels={'to_hop_mon': 'T·ªï h·ª£p', 'diem_chuan': 'ƒêi·ªÉm chu·∫©n'},\n",
    "                box=True, points='outliers')\n",
    "\n",
    "fig.update_layout(height=500, showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12578e7b",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 4. Configuration & Block Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f86953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·ªï h·ª£p m√¥n Y D∆∞·ª£c\n",
    "BLOCK_MAP = {\n",
    "    'A00': ['toan', 'vat_ly', 'hoa_hoc'],\n",
    "    'A01': ['toan', 'vat_ly', 'ngoai_ngu'],\n",
    "    'A02': ['toan', 'vat_ly', 'sinh_hoc'],\n",
    "    'B00': ['toan', 'hoa_hoc', 'sinh_hoc'],\n",
    "    'B08': ['toan', 'sinh_hoc', 'ngoai_ngu'],\n",
    "    'D01': ['toan', 'ngu_van', 'ngoai_ngu'],\n",
    "    'D07': ['toan', 'hoa_hoc', 'ngoai_ngu'],\n",
    "    'D08': ['toan', 'sinh_hoc', 'ngoai_ngu'],\n",
    "    'D13': ['toan', 'ngu_van', 'sinh_hoc'],\n",
    "}\n",
    "\n",
    "print(\"üìö T·ªî H·ª¢P M√îN Y D∆Ø·ª¢C\")\n",
    "print(\"=\"*50)\n",
    "for block, subjects in BLOCK_MAP.items():\n",
    "    subject_names = [s.replace('_', ' ').title() for s in subjects]\n",
    "    print(f\"  {block}: {' + '.join(subject_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f726e8da",
   "metadata": {},
   "source": [
    "## üìä 5. Build Percentile Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6492c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_percentile_lookup(score_folder):\n",
    "    \"\"\"X√¢y d·ª±ng b·∫£ng tra c·ª©u percentile t·ª´ ƒëi·ªÉm thi THPT\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä X√ÇY D·ª∞NG PH√ÇN PH·ªêI ƒêI·ªÇM THI\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    lookup_dict = {}\n",
    "    year_stats = []\n",
    "    files_list = glob.glob(os.path.join(score_folder, \"*.csv\"))\n",
    "    \n",
    "    if not files_list:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file trong {score_folder}\")\n",
    "        return {}, []\n",
    "\n",
    "    print(f\"üìÅ T√¨m th·∫•y {len(files_list)} file\")\n",
    "\n",
    "    for file_path in sorted(files_list):\n",
    "        filename = os.path.basename(file_path)\n",
    "        try:\n",
    "            year = int(re.search(r'\\d{4}', filename).group())\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nüìÖ NƒÉm {year}:\", end=\" \")\n",
    "        all_cols = list(set([col for cols in BLOCK_MAP.values() for col in cols]))\n",
    "        \n",
    "        try:\n",
    "            chunks = pd.read_csv(file_path, usecols=all_cols, chunksize=200000)\n",
    "        except ValueError:\n",
    "            print(\"‚ö†Ô∏è B·ªè qua\")\n",
    "            continue\n",
    "\n",
    "        block_data = {k: [] for k in BLOCK_MAP.keys()}\n",
    "        total_students = 0\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            total_students += len(chunk)\n",
    "            for block, cols in BLOCK_MAP.items():\n",
    "                if not all(col in chunk.columns for col in cols):\n",
    "                    continue\n",
    "                temp = chunk.dropna(subset=cols)\n",
    "                if not temp.empty:\n",
    "                    scores = temp[cols].sum(axis=1).tolist()\n",
    "                    block_data[block].extend(scores)\n",
    "        \n",
    "        blocks_processed = 0\n",
    "        for block, scores in block_data.items():\n",
    "            if not scores:\n",
    "                continue\n",
    "            scores_np = np.array(scores)\n",
    "            scores_np.sort()\n",
    "            scores_np = scores_np[::-1]\n",
    "            \n",
    "            df_score = pd.DataFrame({'score': scores_np})\n",
    "            df_score['rank'] = df_score['score'].rank(method='min', ascending=False)\n",
    "            total = len(df_score)\n",
    "            \n",
    "            lookup = df_score.groupby('score')['rank'].min().reset_index()\n",
    "            lookup['percentile'] = (lookup['rank'] / total) * 100\n",
    "            lookup_dict[(year, block)] = lookup.sort_values('score')\n",
    "            blocks_processed += 1\n",
    "        \n",
    "        print(f\"{total_students:,} th√≠ sinh, {blocks_processed} t·ªï h·ª£p ‚úÖ\")\n",
    "        year_stats.append({'NƒÉm': year, 'Th√≠ sinh': total_students, 'T·ªï h·ª£p': blocks_processed})\n",
    "\n",
    "    print(f\"\\n‚úÖ ƒê√£ x·ª≠ l√Ω {len(lookup_dict)} c·∫∑p (NƒÉm, T·ªï h·ª£p)\")\n",
    "    return lookup_dict, pd.DataFrame(year_stats)\n",
    "\n",
    "# Run\n",
    "lookup_map, stats_df = build_percentile_lookup('diem_thi_thptqg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e1331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize th·ªëng k√™ ƒëi·ªÉm thi\n",
    "if not stats_df.empty:\n",
    "    fig = px.bar(stats_df, x='NƒÉm', y='Th√≠ sinh',\n",
    "                 title='üë• S·ªë l∆∞·ª£ng Th√≠ sinh THPT theo NƒÉm',\n",
    "                 text='Th√≠ sinh',\n",
    "                 color='Th√≠ sinh',\n",
    "                 color_continuous_scale='Blues')\n",
    "    fig.update_traces(texttemplate='%{text:,.0f}', textposition='outside')\n",
    "    fig.update_layout(height=400)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a55f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ph√¢n ph·ªëi ƒëi·ªÉm 2025\n",
    "if (2025, 'B00') in lookup_map:\n",
    "    sample = lookup_map[(2025, 'B00')]\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2,\n",
    "                        subplot_titles=['Ph√¢n ph·ªëi ƒëi·ªÉm B00 (2025)', 'ƒêi·ªÉm ‚Üí Percentile'])\n",
    "    \n",
    "    fig.add_trace(go.Histogram(x=sample['score'], nbinsx=50, name='S·ªë l∆∞·ª£ng',\n",
    "                               marker_color='#636EFA'), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=sample['score'], y=sample['percentile'],\n",
    "                             mode='lines', name='Percentile', line=dict(color='#EF553B')),\n",
    "                  row=1, col=2)\n",
    "    \n",
    "    fig.update_layout(height=400, title_text='üìä Ph√¢n t√≠ch Ph·ªï ƒëi·ªÉm B00 - 2025')\n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\nüìã B·∫£ng tra c·ª©u m·∫´u:\")\n",
    "    for score in [27, 25, 23, 20]:\n",
    "        pct = sample[sample['score'] >= score].iloc[0]['percentile']\n",
    "        print(f\"  ƒêi·ªÉm {score} = Top {pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8437f",
   "metadata": {},
   "source": [
    "## üîÑ 6. Normalize Benchmark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc46e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_multiple_blocks(df):\n",
    "    \"\"\"T√°ch d√≤ng c√≥ nhi·ªÅu t·ªï h·ª£p\"\"\"\n",
    "    expanded_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        to_hop = str(row['to_hop_mon']).strip()\n",
    "        blocks = [b.strip() for b in to_hop.split(',')]\n",
    "        for block in blocks:\n",
    "            if block and block in BLOCK_MAP:\n",
    "                new_row = row.copy()\n",
    "                new_row['to_hop_mon'] = block\n",
    "                expanded_rows.append(new_row)\n",
    "    return pd.DataFrame(expanded_rows)\n",
    "\n",
    "def normalize_benchmark(df, lookup_dict):\n",
    "    \"\"\"Quy ƒë·ªïi ƒëi·ªÉm chu·∫©n sang percentile\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä QUY ƒê·ªîI ƒêI·ªÇM CHU·∫®N SANG PERCENTILE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # T√°ch t·ªï h·ª£p\n",
    "    df_expanded = expand_multiple_blocks(df)\n",
    "    print(f\"Sau khi t√°ch t·ªï h·ª£p: {len(df_expanded):,} d√≤ng\")\n",
    "    \n",
    "    # L·ªçc ng√†nh c√≥ 2025\n",
    "    nganh_2025 = df_expanded[df_expanded['nam'] == 2025][['university_id', 'ma_nganh', 'to_hop_mon']].drop_duplicates()\n",
    "    print(f\"Ng√†nh c√≥ data 2025: {len(nganh_2025)}\")\n",
    "    \n",
    "    df_filtered = df_expanded.merge(nganh_2025, on=['university_id', 'ma_nganh', 'to_hop_mon'], how='inner')\n",
    "    print(f\"Sau khi l·ªçc: {len(df_filtered):,} d√≤ng\")\n",
    "    \n",
    "    # Quy ƒë·ªïi\n",
    "    def get_percentile(row):\n",
    "        lookup = lookup_dict.get((row['nam'], str(row['to_hop_mon']).strip()))\n",
    "        if lookup is None:\n",
    "            return np.nan\n",
    "        idx = np.searchsorted(lookup['score'], row['diem_chuan'], side='left')\n",
    "        return lookup.iloc[idx]['percentile'] if idx < len(lookup) else 0.01\n",
    "    \n",
    "    df_filtered['percentile_rank'] = df_filtered.apply(get_percentile, axis=1)\n",
    "    df_clean = df_filtered.dropna(subset=['percentile_rank'])\n",
    "    \n",
    "    print(f\"\\n‚úÖ D·ªØ li·ªáu training: {len(df_clean):,} d√≤ng\")\n",
    "    return df_clean\n",
    "\n",
    "# Run\n",
    "df_train = normalize_benchmark(df_benchmark, lookup_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec24576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ƒêi·ªÉm vs Percentile\n",
    "sample_data = df_train.sample(min(3000, len(df_train)))\n",
    "\n",
    "fig = px.scatter(sample_data, x='diem_chuan', y='percentile_rank',\n",
    "                 color='nam', opacity=0.6,\n",
    "                 title='üìä M·ªëi quan h·ªá ƒêi·ªÉm chu·∫©n vs Percentile',\n",
    "                 labels={'diem_chuan': 'ƒêi·ªÉm chu·∫©n', 'percentile_rank': 'Top %', 'nam': 'NƒÉm'})\n",
    "\n",
    "fig.update_layout(height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c260c33",
   "metadata": {},
   "source": [
    "## ü§ñ 7. Advanced Models Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c88bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def calculate_trend(values):\n",
    "    \"\"\"Xu h∆∞·ªõng (slope)\"\"\"\n",
    "    if len(values) < 2:\n",
    "        return 0\n",
    "    x = np.arange(len(values)).reshape(-1, 1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, values)\n",
    "    return model.coef_[0]\n",
    "\n",
    "def calculate_volatility(values):\n",
    "    \"\"\"ƒê·ªô bi·∫øn ƒë·ªông\"\"\"\n",
    "    return np.std(values) if len(values) > 1 else 0\n",
    "\n",
    "def calculate_momentum(values):\n",
    "    \"\"\"ƒê·ªông l∆∞·ª£ng (thay ƒë·ªïi g·∫ßn ƒë√¢y)\"\"\"\n",
    "    return values[-1] - values[-2] if len(values) >= 2 else 0\n",
    "\n",
    "# Prediction Models\n",
    "def predict_weighted_average(values, alpha=2.0):\n",
    "    \"\"\"Weighted Average v·ªõi exponential weights\"\"\"\n",
    "    n = len(values)\n",
    "    if n == 0:\n",
    "        return None, None\n",
    "    weights = np.exp(np.linspace(0, alpha, n))\n",
    "    pred = np.sum(values * weights) / weights.sum()\n",
    "    weighted_var = np.sum(weights * (values - pred)**2) / weights.sum()\n",
    "    ci = 1.96 * np.sqrt(weighted_var) if weighted_var > 0 else 0\n",
    "    return pred, ci\n",
    "\n",
    "def predict_ets(values):\n",
    "    \"\"\"Exponential Smoothing\"\"\"\n",
    "    try:\n",
    "        if len(values) < 4:\n",
    "            return None, None\n",
    "        model = ExponentialSmoothing(values, trend='add', seasonal=None)\n",
    "        fit = model.fit()\n",
    "        pred = fit.forecast(1)[0]\n",
    "        residuals = values - fit.fittedvalues\n",
    "        ci = 1.96 * np.std(residuals)\n",
    "        return pred, ci\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "def predict_arima(values):\n",
    "    \"\"\"ARIMA(1,1,1)\"\"\"\n",
    "    try:\n",
    "        if len(values) < 5:\n",
    "            return None, None\n",
    "        model = ARIMA(values, order=(1, 1, 1))\n",
    "        fit = model.fit()\n",
    "        forecast = fit.get_forecast(steps=1)\n",
    "        pred = forecast.predicted_mean[0]\n",
    "        ci = forecast.conf_int().iloc[0, 1] - pred\n",
    "        return pred, ci\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "def predict_linear_regression(values):\n",
    "    \"\"\"Linear Regression\"\"\"\n",
    "    try:\n",
    "        if len(values) < 3:\n",
    "            return None, None\n",
    "        x = np.arange(len(values)).reshape(-1, 1)\n",
    "        model = LinearRegression()\n",
    "        model.fit(x, values)\n",
    "        pred = model.predict([[len(values)]])[0]\n",
    "        residuals = values - model.predict(x)\n",
    "        ci = 1.96 * np.std(residuals)\n",
    "        return pred, ci\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a 4 models: WA, ETS, ARIMA, LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(values):\n",
    "    \"\"\"Ensemble prediction - K·∫øt h·ª£p nhi·ªÅu models\"\"\"\n",
    "    predictions = {}\n",
    "    confidence_intervals = {}\n",
    "    \n",
    "    # Run all models\n",
    "    for name, func in [('WA', predict_weighted_average), ('ETS', predict_ets),\n",
    "                       ('ARIMA', predict_arima), ('LR', predict_linear_regression)]:\n",
    "        pred, ci = func(values)\n",
    "        if pred is not None:\n",
    "            predictions[name] = pred\n",
    "            confidence_intervals[name] = ci if ci and ci > 0 else 0.1\n",
    "    \n",
    "    if not predictions:\n",
    "        return np.mean(values), np.std(values) * 1.96, 'MEAN', {}\n",
    "    \n",
    "    # Weighted ensemble (weight = 1/CI)\n",
    "    total_weight = 0\n",
    "    weighted_pred = 0\n",
    "    \n",
    "    for model, pred in predictions.items():\n",
    "        weight = 1 / confidence_intervals[model]\n",
    "        weighted_pred += pred * weight\n",
    "        total_weight += weight\n",
    "    \n",
    "    ensemble_pred = weighted_pred / total_weight if total_weight > 0 else np.mean(list(predictions.values()))\n",
    "    best_model = min(confidence_intervals.keys(), key=lambda k: confidence_intervals[k])\n",
    "    ensemble_ci = np.mean(list(confidence_intervals.values()))\n",
    "    \n",
    "    return ensemble_pred, ensemble_ci, best_model, predictions\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a Ensemble Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f9d1b4",
   "metadata": {},
   "source": [
    "## üèãÔ∏è 8. Training v·ªõi Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e7c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv(values, n_splits=3):\n",
    "    \"\"\"Time Series Cross-Validation\"\"\"\n",
    "    if len(values) < n_splits + 2:\n",
    "        return None\n",
    "    \n",
    "    errors = defaultdict(list)\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        split_point = len(values) - n_splits + i\n",
    "        if split_point < 3:\n",
    "            continue\n",
    "        \n",
    "        train = values[:split_point]\n",
    "        test = values[split_point]\n",
    "        \n",
    "        for name, func in [('WA', predict_weighted_average), ('ETS', predict_ets),\n",
    "                           ('ARIMA', predict_arima), ('LR', predict_linear_regression)]:\n",
    "            pred, _ = func(train)\n",
    "            if pred is not None:\n",
    "                errors[name].append(abs(pred - test))\n",
    "    \n",
    "    return {k: np.mean(v) if v else float('inf') for k, v in errors.items()}\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a Time Series Cross-Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74827f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_advanced(df_labeled):\n",
    "    \"\"\"Advanced Training v·ªõi Ensemble & CV\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ü§ñ ADVANCED MODEL TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    groups = df_labeled.groupby(['university_id', 'ma_nganh', 'to_hop_mon'])\n",
    "    \n",
    "    forecast_results = {}\n",
    "    confidence_results = {}\n",
    "    model_selection = {}\n",
    "    feature_store = {}\n",
    "    \n",
    "    stats = defaultdict(int)\n",
    "    all_errors = defaultdict(list)\n",
    "    \n",
    "    total_groups = len(groups)\n",
    "    print(f\"\\nüìä Training {total_groups} nh√≥m ng√†nh...\\n\")\n",
    "    \n",
    "    for i, (name, group) in enumerate(groups):\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  ‚è≥ {i+1}/{total_groups} ({100*(i+1)/total_groups:.0f}%)\", end=\"\\r\")\n",
    "        \n",
    "        group = group.sort_values('nam')\n",
    "        percentiles = group['percentile_rank'].values\n",
    "        \n",
    "        # Extract features\n",
    "        features = {\n",
    "            'mean': np.mean(percentiles),\n",
    "            'std': np.std(percentiles) if len(percentiles) > 1 else 0,\n",
    "            'trend': calculate_trend(percentiles),\n",
    "            'volatility': calculate_volatility(percentiles),\n",
    "            'momentum': calculate_momentum(percentiles),\n",
    "            'n_years': len(percentiles)\n",
    "        }\n",
    "        feature_store[name] = features\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_errors = time_series_cv(percentiles)\n",
    "        \n",
    "        if cv_errors and len(percentiles) >= 4:\n",
    "            for model, err in cv_errors.items():\n",
    "                all_errors[model].append(err)\n",
    "            \n",
    "            # Ensemble prediction\n",
    "            pred, ci, best_model, _ = ensemble_predict(percentiles)\n",
    "        else:\n",
    "            pred, ci = predict_weighted_average(percentiles)\n",
    "            if pred is None:\n",
    "                pred = np.mean(percentiles)\n",
    "                ci = np.std(percentiles) * 1.96\n",
    "            best_model = 'WA'\n",
    "            stats['WA_only'] += 1\n",
    "        \n",
    "        # Safety factor\n",
    "        adjusted_pred = pred * 0.95\n",
    "        \n",
    "        forecast_results[name] = adjusted_pred\n",
    "        confidence_results[name] = ci if ci else 0\n",
    "        model_selection[name] = best_model\n",
    "        stats[best_model] += 1\n",
    "    \n",
    "    print(f\"\\n\\n‚úÖ ƒê√£ train {len(forecast_results)} nh√≥m!\")\n",
    "    print(f\"\\nüìä Model Selection:\")\n",
    "    for model, count in sorted(stats.items()):\n",
    "        print(f\"   {model}: {count} ({100*count/total_groups:.1f}%)\")\n",
    "    \n",
    "    if all_errors:\n",
    "        print(f\"\\nüìâ CV Error (MAE):\")\n",
    "        for model, errors in all_errors.items():\n",
    "            if errors:\n",
    "                print(f\"   {model}: {np.mean(errors):.3f} ¬± {np.std(errors):.3f}\")\n",
    "    \n",
    "    analytics = {\n",
    "        'predictions': forecast_results,\n",
    "        'confidence_intervals': confidence_results,\n",
    "        'model_selection': model_selection,\n",
    "        'features': feature_store,\n",
    "        'statistics': dict(stats),\n",
    "        'cv_errors': {k: float(np.mean(v)) for k, v in all_errors.items() if v}\n",
    "    }\n",
    "    \n",
    "    return forecast_results, confidence_results, analytics\n",
    "\n",
    "# RUN TRAINING\n",
    "model_2026, confidence_2026, analytics = train_advanced(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df62744c",
   "metadata": {},
   "source": [
    "## üìä 9. Model Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc7a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 Model Selection Pie Chart\n",
    "model_counts = analytics['statistics']\n",
    "models = [k for k in model_counts.keys() if not k.endswith('_only')]\n",
    "counts = [model_counts[k] for k in models]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(\n",
    "    labels=models,\n",
    "    values=counts,\n",
    "    hole=0.4,\n",
    "    marker_colors=['#636EFA', '#EF553B', '#00CC96', '#AB63FA', '#FFA15A']\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='üèÜ Model Selection Distribution',\n",
    "    height=400\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2 CV Error Comparison\n",
    "cv_errors = analytics.get('cv_errors', {})\n",
    "\n",
    "if cv_errors:\n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=list(cv_errors.values()),\n",
    "        y=list(cv_errors.keys()),\n",
    "        orientation='h',\n",
    "        marker=dict(color=['#636EFA', '#EF553B', '#00CC96', '#AB63FA'][:len(cv_errors)])\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='üìâ Cross-Validation Error (MAE)',\n",
    "        xaxis_title='Mean Absolute Error',\n",
    "        height=400\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 Confidence Interval Distribution\n",
    "ci_values = list(confidence_2026.values())\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=ci_values, nbinsx=40, marker_color='#636EFA', opacity=0.7))\n",
    "fig.add_vline(x=np.mean(ci_values), line_dash=\"dash\", line_color=\"red\",\n",
    "              annotation_text=f\"Mean: {np.mean(ci_values):.2f}\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='üìä Confidence Interval Distribution',\n",
    "    xaxis_title='Confidence Interval (percentile points)',\n",
    "    yaxis_title='Frequency',\n",
    "    height=400\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.4 Prediction Distribution\n",
    "pred_values = list(model_2026.values())\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=pred_values, nbinsx=40, marker_color='#00CC96', opacity=0.7))\n",
    "fig.add_vline(x=np.mean(pred_values), line_dash=\"dash\", line_color=\"red\",\n",
    "              annotation_text=f\"Mean: {np.mean(pred_values):.2f}\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='üéØ Predicted Percentile Distribution (2026)',\n",
    "    xaxis_title='Predicted Percentile (Top %)',\n",
    "    yaxis_title='Frequency',\n",
    "    height=400\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f06c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.5 Feature Importance - Trend vs Volatility\n",
    "features_df = pd.DataFrame([\n",
    "    {'key': k, 'trend': v['trend'], 'volatility': v['volatility'], 'n_years': v['n_years']}\n",
    "    for k, v in analytics['features'].items()\n",
    "])\n",
    "\n",
    "fig = px.scatter(features_df.sample(min(500, len(features_df))),\n",
    "                 x='trend', y='volatility', size='n_years',\n",
    "                 title='üìà Feature Analysis: Trend vs Volatility',\n",
    "                 labels={'trend': 'Trend (Xu h∆∞·ªõng)', 'volatility': 'Volatility (Bi·∫øn ƒë·ªông)'},\n",
    "                 color='n_years', color_continuous_scale='Viridis')\n",
    "\n",
    "fig.update_layout(height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6e14e4",
   "metadata": {},
   "source": [
    "## üíæ 10. Save & Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d867a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_artifacts\n",
    "\n",
    "# Save\n",
    "joblib.dump(model_2026, 'model_artifacts/university_ranking_model_2026.pkl')\n",
    "\n",
    "lookup_2025 = {k: v for k, v in lookup_map.items() if k[0] == 2025}\n",
    "joblib.dump(lookup_2025, 'model_artifacts/score_distribution_2025.pkl')\n",
    "\n",
    "joblib.dump(analytics, 'model_artifacts/model_analytics.pkl')\n",
    "\n",
    "# Save confidence intervals\n",
    "joblib.dump(confidence_2026, 'model_artifacts/confidence_intervals.pkl')\n",
    "\n",
    "print(\"‚úÖ ƒê√£ l∆∞u t·∫•t c·∫£ files!\")\n",
    "print(f\"   - Model: {len(model_2026)} ng√†nh\")\n",
    "print(f\"   - Lookup: {len(lookup_2025)} t·ªï h·ª£p\")\n",
    "print(f\"   - Analytics: Full statistics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed05519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download\n",
    "print(\"üì• Downloading...\")\n",
    "files.download('model_artifacts/university_ranking_model_2026.pkl')\n",
    "files.download('model_artifacts/score_distribution_2025.pkl')\n",
    "files.download('model_artifacts/model_analytics.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a594d86",
   "metadata": {},
   "source": [
    "## üéØ 11. Demo: T√¨m Ng√†nh Ph√π H·ª£p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8afc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_percentile(score, block, lookup_dict):\n",
    "    \"\"\"Quy ƒë·ªïi ƒëi·ªÉm user sang percentile\"\"\"\n",
    "    key = (2025, block)\n",
    "    if key not in lookup_dict:\n",
    "        return None\n",
    "    table = lookup_dict[key]\n",
    "    idx = np.searchsorted(table['score'], score, side='left')\n",
    "    return table.iloc[idx]['percentile'] if idx < len(table) else 0.01\n",
    "\n",
    "def find_suitable_majors(student_scores, model, confidence, lookup, df_info, priority=0, top_n=30):\n",
    "    \"\"\"T√¨m ng√†nh ph√π h·ª£p v·ªõi confidence interval\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for key, predicted_percentile in model.items():\n",
    "        university_id, ma_nganh, block = key\n",
    "        \n",
    "        if block not in BLOCK_MAP:\n",
    "            continue\n",
    "        \n",
    "        subjects = BLOCK_MAP[block]\n",
    "        if not all(s in student_scores for s in subjects):\n",
    "            continue\n",
    "        \n",
    "        diem_cua_ban = sum(student_scores[s] for s in subjects) + priority\n",
    "        student_percentile = get_user_percentile(diem_cua_ban, block, lookup)\n",
    "        \n",
    "        if student_percentile is None:\n",
    "            continue\n",
    "        \n",
    "        khoang_cach = predicted_percentile - student_percentile\n",
    "        ci = confidence.get(key, 0)\n",
    "        \n",
    "        # L·∫•y info\n",
    "        info = df_info[(df_info['university_id'] == university_id) & (df_info['ma_nganh'] == ma_nganh)]\n",
    "        if info.empty:\n",
    "            continue\n",
    "        info = info.iloc[0]\n",
    "        \n",
    "        # ƒê√°nh gi√°\n",
    "        if khoang_cach > 10:\n",
    "            kha_nang = \"üü¢ R·∫•t cao\"\n",
    "            score = 5\n",
    "        elif khoang_cach > 2:\n",
    "            kha_nang = \"üü¢ Cao\"\n",
    "            score = 4\n",
    "        elif khoang_cach > 0:\n",
    "            kha_nang = \"üü° Trung b√¨nh\"\n",
    "            score = 3\n",
    "        elif khoang_cach > -2:\n",
    "            kha_nang = \"üü† Th·∫•p\"\n",
    "            score = 2\n",
    "        else:\n",
    "            kha_nang = \"üî¥ R·∫•t th·∫•p\"\n",
    "            score = 1\n",
    "        \n",
    "        results.append({\n",
    "            'Tr∆∞·ªùng': info['ten_truong'],\n",
    "            'Ng√†nh': info['ten_nganh'],\n",
    "            'T·ªï h·ª£p': block,\n",
    "            'ƒêi·ªÉm b·∫°n': diem_cua_ban,\n",
    "            'Kh·∫£ nƒÉng': kha_nang,\n",
    "            'CI ¬±': round(ci, 1),\n",
    "            'score': score\n",
    "        })\n",
    "    \n",
    "    df_result = pd.DataFrame(results).sort_values('score', ascending=False)\n",
    "    return df_result.head(top_n)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a h√†m t√¨m ng√†nh!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f108f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NH·∫¨P ƒêI·ªÇM C·ª¶A B·∫†N ===\n",
    "print(\"=\"*50)\n",
    "print(\"üéì DEMO: T√åM NG√ÄNH PH√ô H·ª¢P\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "student_scores = {\n",
    "    'toan': 8.5,\n",
    "    'vat_ly': 8.0,\n",
    "    'hoa_hoc': 8.5,\n",
    "    'sinh_hoc': 7.5,\n",
    "    'ngu_van': 7.0,\n",
    "    'ngoai_ngu': 8.0\n",
    "}\n",
    "priority = 0\n",
    "\n",
    "print(\"\\nüìù ƒêi·ªÉm c·ªßa b·∫°n:\")\n",
    "for s, score in student_scores.items():\n",
    "    print(f\"   {s}: {score}\")\n",
    "\n",
    "print(\"\\nüìä ƒêi·ªÉm t·ªï h·ª£p:\")\n",
    "for block, subjects in BLOCK_MAP.items():\n",
    "    total = sum(student_scores.get(s, 0) for s in subjects) + priority\n",
    "    print(f\"   {block}: {total:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0522ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√¨m ng√†nh\n",
    "suitable = find_suitable_majors(student_scores, model_2026, confidence_2026, lookup_2025, df_benchmark, priority)\n",
    "\n",
    "if not suitable.empty:\n",
    "    print(\"\\nüéØ NG√ÄNH PH√ô H·ª¢P:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    display(suitable[['Tr∆∞·ªùng', 'Ng√†nh', 'T·ªï h·ª£p', 'ƒêi·ªÉm b·∫°n', 'Kh·∫£ nƒÉng', 'CI ¬±']])\n",
    "    \n",
    "    # Pie chart\n",
    "    chance_counts = suitable['Kh·∫£ nƒÉng'].value_counts()\n",
    "    fig = px.pie(values=chance_counts.values, names=chance_counts.index,\n",
    "                 title='üìä Ph√¢n b·ªë C∆° h·ªôi ƒë·∫≠u', hole=0.4)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"‚ùå Kh√¥ng t√¨m th·∫•y ng√†nh ph√π h·ª£p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ff875",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ HO√ÄN TH√ÄNH!\n",
    "\n",
    "### T√≥m t·∫Øt:\n",
    "1. ‚úÖ **5 Models**: WA, ETS, ARIMA, LR, Ensemble\n",
    "2. ‚úÖ **Cross-Validation**: Time Series CV\n",
    "3. ‚úÖ **Confidence Interval**: Kho·∫£ng tin c·∫≠y 95%\n",
    "4. ‚úÖ **Feature Engineering**: Trend, Volatility, Momentum\n",
    "5. ‚úÖ **Advanced Visualization**: Interactive Plotly charts\n",
    "\n",
    "### Files output:\n",
    "- `university_ranking_model_2026.pkl` - Predictions\n",
    "- `score_distribution_2025.pkl` - Percentile lookup\n",
    "- `model_analytics.pkl` - Full analytics\n",
    "- `confidence_intervals.pkl` - CI for each major\n",
    "\n",
    "---\n",
    "**¬© 2025 Advanced University Admission Predictor V2.0**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
