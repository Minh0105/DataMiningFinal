{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "59dfa56f",
      "metadata": {
        "id": "59dfa56f"
      },
      "source": [
        "# üéì ADVANCED UNIVERSITY ADMISSION PREDICTION SYSTEM\n",
        "## H·ªá th·ªëng D·ª± b√°o ƒêi·ªÉm chu·∫©n ƒê·∫°i h·ªçc Y D∆∞·ª£c 2026\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ H·ªá th·ªëng n√†y l√†m g√¨?\n",
        "\n",
        "**2 M·ª•c ti√™u ch√≠nh:**\n",
        "\n",
        "1. **üìä D·ª± ƒëo√°n PERCENTILE ƒëi·ªÉm chu·∫©n 2026**\n",
        "   - S·ª≠ d·ª•ng: Time Series Models (WA, ETS, ARIMA, LR)\n",
        "   - Ensemble Learning + Cross-Validation\n",
        "   - Output: \"Ng√†nh Y khoa s·∫Ω l·∫•y Top 0.5% nƒÉm 2026\"\n",
        "\n",
        "2. **üéØ D·ª± ƒëo√°n X√ÅC SU·∫§T ƒê·∫¨U c·ªßa th√≠ sinh**\n",
        "   - S·ª≠ d·ª•ng: Classification Models (Random Forest, Gradient Boosting)\n",
        "   - **KH√îNG d√πng `gap` (ƒëi·ªÉm - ƒëi·ªÉm_chu·∫©n)** v√¨ ch∆∞a bi·∫øt ƒëi·ªÉm chu·∫©n 2026!\n",
        "   - Output: \"V·ªõi 27 ƒëi·ªÉm B00, b·∫°n c√≥ 65% c∆° h·ªôi ƒë·∫≠u Y khoa\"\n",
        "\n",
        "---\n",
        "\n",
        "### üí° T·∫°i sao c√≥ v1, v2, v3?\n",
        "\n",
        "| Version | C√°ch ti·∫øp c·∫≠n | V·∫•n ƒë·ªÅ |\n",
        "|---------|---------------|--------|\n",
        "| v1, v2 | D√πng `gap = ƒëi·ªÉm - ƒëi·ªÉm_chu·∫©n` | ‚ùå **Data leakage!** NƒÉm 2026 ch∆∞a c√≥ ƒëi·ªÉm chu·∫©n |\n",
        "| **v3** | Ch·ªâ d√πng `percentile` + pattern l·ªãch s·ª≠ | ‚úÖ **Realistic!** |\n",
        "\n",
        "---\n",
        "\n",
        "**Author**: AI University Admission Predictor V2.0  \n",
        "**Data**: ƒêi·ªÉm chu·∫©n Y D∆∞·ª£c 2018-2025 + ƒêi·ªÉm thi THPT 2018-2025"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61f94c90",
      "metadata": {
        "id": "61f94c90"
      },
      "source": [
        "## üì¶ 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d58a514a",
      "metadata": {
        "id": "d58a514a"
      },
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt (ch·∫°y 1 l·∫ßn)\n",
        "# !pip install statsmodels plotly kaleido scikit-learn pandas numpy matplotlib seaborn joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "315051f4",
      "metadata": {
        "id": "315051f4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import joblib\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "# ML & Stats\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, roc_auc_score\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "print(\"‚úÖ Import th√†nh c√¥ng!\")\n",
        "print(f\"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99ba85f8",
      "metadata": {
        "id": "99ba85f8"
      },
      "source": [
        "## üì§ 2. Load Data (Local - VS Code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d3b17a",
      "metadata": {
        "id": "b0d3b17a"
      },
      "outputs": [],
      "source": [
        "# Config paths - ƒêi·ªÅu ch·ªânh n·∫øu c·∫ßn\n",
        "SCORE_FOLDER = 'diem_thi_thptqg'\n",
        "BENCHMARK_FILE = 'diem_chuan_cleaned.csv'\n",
        "OUTPUT_DIR = 'model_artifacts'\n",
        "\n",
        "# T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ Score folder: {SCORE_FOLDER}\")\n",
        "print(f\"üìÅ Benchmark file: {BENCHMARK_FILE}\")\n",
        "print(f\"üìÅ Output dir: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10fe5504",
      "metadata": {
        "id": "10fe5504"
      },
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra files c√≥ t·ªìn t·∫°i kh√¥ng\n",
        "print(\"üìã Ki·ªÉm tra files...\")\n",
        "\n",
        "# Check benchmark\n",
        "if os.path.exists(BENCHMARK_FILE):\n",
        "    print(f\"  ‚úÖ {BENCHMARK_FILE}\")\n",
        "else:\n",
        "    print(f\"  ‚ùå {BENCHMARK_FILE} - KH√îNG T√åM TH·∫§Y!\")\n",
        "\n",
        "# Check score files\n",
        "score_files = glob.glob(os.path.join(SCORE_FOLDER, \"*.csv\"))\n",
        "print(f\"\\nüìä T√¨m th·∫•y {len(score_files)} file ƒëi·ªÉm thi:\")\n",
        "for f in sorted(score_files):\n",
        "    print(f\"  ‚úÖ {os.path.basename(f)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a84eb8c",
      "metadata": {
        "id": "2a84eb8c"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "df_benchmark = pd.read_csv('diem_chuan_cleaned.csv')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"üìä TH√îNG TIN D·ªÆ LI·ªÜU\")\n",
        "print(\"=\"*50)\n",
        "print(f\"S·ªë d√≤ng: {len(df_benchmark):,}\")\n",
        "print(f\"S·ªë c·ªôt: {len(df_benchmark.columns)}\")\n",
        "print(f\"C√°c nƒÉm: {sorted(df_benchmark['nam'].unique())}\")\n",
        "print(f\"S·ªë tr∆∞·ªùng: {df_benchmark['university_id'].nunique()}\")\n",
        "print(f\"S·ªë ng√†nh: {df_benchmark['ma_nganh'].nunique()}\")\n",
        "\n",
        "df_benchmark.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9afb47b4",
      "metadata": {
        "id": "9afb47b4"
      },
      "source": [
        "## üìä 3. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "792d7538",
      "metadata": {
        "id": "792d7538"
      },
      "outputs": [],
      "source": [
        "# 3.1 Overview Statistics\n",
        "print(\"üìà TH·ªêNG K√ä T·ªîNG QUAN\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "yearly_stats = df_benchmark.groupby('nam').agg({\n",
        "    'diem_chuan': ['mean', 'std', 'min', 'max', 'count'],\n",
        "    'university_id': 'nunique',\n",
        "    'ma_nganh': 'nunique'\n",
        "}).round(2)\n",
        "\n",
        "yearly_stats.columns = ['ƒêi·ªÉm TB', 'Std', 'Min', 'Max', 'S·ªë d√≤ng', 'S·ªë tr∆∞·ªùng', 'S·ªë ng√†nh']\n",
        "yearly_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909b67ea",
      "metadata": {
        "id": "909b67ea"
      },
      "outputs": [],
      "source": [
        "# 3.2 Interactive Box Plot - ƒêi·ªÉm theo nƒÉm\n",
        "fig = px.box(df_benchmark, x='nam', y='diem_chuan',\n",
        "             color='nam',\n",
        "             title='üìä Ph√¢n b·ªë ƒêi·ªÉm chu·∫©n theo NƒÉm (2018-2025)',\n",
        "             labels={'nam': 'NƒÉm', 'diem_chuan': 'ƒêi·ªÉm chu·∫©n'},\n",
        "             color_discrete_sequence=px.colors.sequential.Viridis)\n",
        "\n",
        "fig.update_layout(\n",
        "    height=500,\n",
        "    showlegend=False,\n",
        "    xaxis_title=\"NƒÉm\",\n",
        "    yaxis_title=\"ƒêi·ªÉm chu·∫©n\"\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "947bb086",
      "metadata": {
        "id": "947bb086"
      },
      "outputs": [],
      "source": [
        "# 3.3 Top 10 tr∆∞·ªùng ƒëi·ªÉm cao nh·∫•t 2025\n",
        "df_2025 = df_benchmark[df_benchmark['nam'] == 2025]\n",
        "top_schools = df_2025.groupby('ten_truong')['diem_chuan'].max().sort_values(ascending=True).tail(10)\n",
        "\n",
        "fig = go.Figure(go.Bar(\n",
        "    x=top_schools.values,\n",
        "    y=top_schools.index,\n",
        "    orientation='h',\n",
        "    marker=dict(color=top_schools.values, colorscale='Reds')\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='üèÜ Top 10 Tr∆∞·ªùng c√≥ ƒêi·ªÉm chu·∫©n cao nh·∫•t (2025)',\n",
        "    xaxis_title='ƒêi·ªÉm chu·∫©n',\n",
        "    yaxis_title='',\n",
        "    height=500\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84d9365a",
      "metadata": {
        "id": "84d9365a"
      },
      "outputs": [],
      "source": [
        "# 3.4 Xu h∆∞·ªõng ƒëi·ªÉm chu·∫©n theo th·ªùi gian - Top 5 ng√†nh\n",
        "top_nganh = df_benchmark.groupby('ten_nganh').size().sort_values(ascending=False).head(5).index.tolist()\n",
        "df_top = df_benchmark[df_benchmark['ten_nganh'].isin(top_nganh)]\n",
        "trend = df_top.groupby(['nam', 'ten_nganh'])['diem_chuan'].mean().reset_index()\n",
        "\n",
        "fig = px.line(trend, x='nam', y='diem_chuan', color='ten_nganh',\n",
        "              title='üìà Xu h∆∞·ªõng ƒêi·ªÉm chu·∫©n Top 5 Ng√†nh (2018-2025)',\n",
        "              labels={'nam': 'NƒÉm', 'diem_chuan': 'ƒêi·ªÉm chu·∫©n TB', 'ten_nganh': 'Ng√†nh'},\n",
        "              markers=True)\n",
        "\n",
        "fig.update_layout(height=500, legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.3))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fde0a319",
      "metadata": {
        "id": "fde0a319"
      },
      "outputs": [],
      "source": [
        "# 3.5 Heatmap - ƒêi·ªÉm chu·∫©n theo Tr∆∞·ªùng x NƒÉm\n",
        "pivot = df_benchmark.pivot_table(values='diem_chuan', index='ten_truong', columns='nam', aggfunc='max')\n",
        "pivot = pivot.loc[pivot.max(axis=1).sort_values(ascending=False).head(12).index]\n",
        "\n",
        "fig = px.imshow(pivot,\n",
        "                title='üî• Heatmap ƒêi·ªÉm chu·∫©n Max theo Tr∆∞·ªùng x NƒÉm',\n",
        "                labels=dict(x=\"NƒÉm\", y=\"Tr∆∞·ªùng\", color=\"ƒêi·ªÉm\"),\n",
        "                color_continuous_scale='RdYlGn',\n",
        "                aspect='auto')\n",
        "\n",
        "fig.update_layout(height=500)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f145859",
      "metadata": {
        "id": "9f145859"
      },
      "outputs": [],
      "source": [
        "# 3.6 Violin Plot - Ph√¢n b·ªë ƒëi·ªÉm theo t·ªï h·ª£p\n",
        "top_blocks = df_benchmark['to_hop_mon'].value_counts().head(8).index.tolist()\n",
        "df_blocks = df_benchmark[df_benchmark['to_hop_mon'].isin(top_blocks)]\n",
        "\n",
        "fig = px.violin(df_blocks, x='to_hop_mon', y='diem_chuan', color='to_hop_mon',\n",
        "                title='üéª Ph√¢n b·ªë ƒêi·ªÉm chu·∫©n theo T·ªï h·ª£p m√¥n',\n",
        "                labels={'to_hop_mon': 'T·ªï h·ª£p', 'diem_chuan': 'ƒêi·ªÉm chu·∫©n'},\n",
        "                box=True, points='outliers')\n",
        "\n",
        "fig.update_layout(height=500, showlegend=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12578e7b",
      "metadata": {
        "id": "12578e7b"
      },
      "source": [
        "## ‚öôÔ∏è 4. Configuration & Block Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f86953c",
      "metadata": {
        "id": "1f86953c"
      },
      "outputs": [],
      "source": [
        "# T·ªï h·ª£p m√¥n Y D∆∞·ª£c\n",
        "BLOCK_MAP = {\n",
        "    'A00': ['toan', 'vat_ly', 'hoa_hoc'],\n",
        "    'A01': ['toan', 'vat_ly', 'ngoai_ngu'],\n",
        "    'A02': ['toan', 'vat_ly', 'sinh_hoc'],\n",
        "    'B00': ['toan', 'hoa_hoc', 'sinh_hoc'],\n",
        "    'B08': ['toan', 'sinh_hoc', 'ngoai_ngu'],\n",
        "    'D01': ['toan', 'ngu_van', 'ngoai_ngu'],\n",
        "    'D07': ['toan', 'hoa_hoc', 'ngoai_ngu'],\n",
        "    'D08': ['toan', 'sinh_hoc', 'ngoai_ngu'],\n",
        "    'D13': ['toan', 'ngu_van', 'sinh_hoc'],\n",
        "}\n",
        "\n",
        "print(\"üìö T·ªî H·ª¢P M√îN Y D∆Ø·ª¢C\")\n",
        "print(\"=\"*50)\n",
        "for block, subjects in BLOCK_MAP.items():\n",
        "    subject_names = [s.replace('_', ' ').title() for s in subjects]\n",
        "    print(f\"  {block}: {' + '.join(subject_names)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f726e8da",
      "metadata": {
        "id": "f726e8da"
      },
      "source": [
        "## üìä 5. Build Percentile Lookup Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6492c873",
      "metadata": {
        "id": "6492c873"
      },
      "outputs": [],
      "source": [
        "def build_percentile_lookup(score_folder):\n",
        "    \"\"\"X√¢y d·ª±ng b·∫£ng tra c·ª©u percentile t·ª´ ƒëi·ªÉm thi THPT\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä X√ÇY D·ª∞NG PH√ÇN PH·ªêI ƒêI·ªÇM THI\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    lookup_dict = {}\n",
        "    year_stats = []\n",
        "    files_list = glob.glob(os.path.join(score_folder, \"*.csv\"))\n",
        "\n",
        "    if not files_list:\n",
        "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file trong {score_folder}\")\n",
        "        return {}, []\n",
        "\n",
        "    print(f\"üìÅ T√¨m th·∫•y {len(files_list)} file\")\n",
        "\n",
        "    for file_path in sorted(files_list):\n",
        "        filename = os.path.basename(file_path)\n",
        "        try:\n",
        "            year = int(re.search(r'\\d{4}', filename).group())\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nüìÖ NƒÉm {year}:\", end=\" \")\n",
        "        all_cols = list(set([col for cols in BLOCK_MAP.values() for col in cols]))\n",
        "\n",
        "        try:\n",
        "            chunks = pd.read_csv(file_path, usecols=all_cols, chunksize=200000)\n",
        "        except ValueError:\n",
        "            print(\"‚ö†Ô∏è B·ªè qua\")\n",
        "            continue\n",
        "\n",
        "        block_data = {k: [] for k in BLOCK_MAP.keys()}\n",
        "        total_students = 0\n",
        "\n",
        "        for chunk in chunks:\n",
        "            total_students += len(chunk)\n",
        "            for block, cols in BLOCK_MAP.items():\n",
        "                if not all(col in chunk.columns for col in cols):\n",
        "                    continue\n",
        "                temp = chunk.dropna(subset=cols)\n",
        "                if not temp.empty:\n",
        "                    scores = temp[cols].sum(axis=1).tolist()\n",
        "                    block_data[block].extend(scores)\n",
        "\n",
        "        blocks_processed = 0\n",
        "        for block, scores in block_data.items():\n",
        "            if not scores:\n",
        "                continue\n",
        "            scores_np = np.array(scores)\n",
        "            scores_np.sort()\n",
        "            scores_np = scores_np[::-1]\n",
        "\n",
        "            df_score = pd.DataFrame({'score': scores_np})\n",
        "            df_score['rank'] = df_score['score'].rank(method='min', ascending=False)\n",
        "            total = len(df_score)\n",
        "\n",
        "            lookup = df_score.groupby('score')['rank'].min().reset_index()\n",
        "            lookup['percentile'] = (lookup['rank'] / total) * 100\n",
        "            lookup_dict[(year, block)] = lookup.sort_values('score')\n",
        "            blocks_processed += 1\n",
        "\n",
        "        print(f\"{total_students:,} th√≠ sinh, {blocks_processed} t·ªï h·ª£p ‚úÖ\")\n",
        "        year_stats.append({'NƒÉm': year, 'Th√≠ sinh': total_students, 'T·ªï h·ª£p': blocks_processed})\n",
        "\n",
        "    print(f\"\\n‚úÖ ƒê√£ x·ª≠ l√Ω {len(lookup_dict)} c·∫∑p (NƒÉm, T·ªï h·ª£p)\")\n",
        "    return lookup_dict, pd.DataFrame(year_stats)\n",
        "\n",
        "# Run\n",
        "lookup_map, stats_df = build_percentile_lookup('diem_thi_thptqg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27e1331c",
      "metadata": {
        "id": "27e1331c"
      },
      "outputs": [],
      "source": [
        "# Visualize th·ªëng k√™ ƒëi·ªÉm thi\n",
        "if not stats_df.empty:\n",
        "    fig = px.bar(stats_df, x='NƒÉm', y='Th√≠ sinh',\n",
        "                 title='üë• S·ªë l∆∞·ª£ng Th√≠ sinh THPT theo NƒÉm',\n",
        "                 text='Th√≠ sinh',\n",
        "                 color='Th√≠ sinh',\n",
        "                 color_continuous_scale='Blues')\n",
        "    fig.update_traces(texttemplate='%{text:,.0f}', textposition='outside')\n",
        "    fig.update_layout(height=400)\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a55f4b0",
      "metadata": {
        "id": "5a55f4b0"
      },
      "outputs": [],
      "source": [
        "# Visualize ph√¢n ph·ªëi ƒëi·ªÉm 2025\n",
        "if (2025, 'B00') in lookup_map:\n",
        "    sample = lookup_map[(2025, 'B00')]\n",
        "\n",
        "    fig = make_subplots(rows=1, cols=2,\n",
        "                        subplot_titles=['Ph√¢n ph·ªëi ƒëi·ªÉm B00 (2025)', 'ƒêi·ªÉm ‚Üí Percentile'])\n",
        "\n",
        "    fig.add_trace(go.Histogram(x=sample['score'], nbinsx=50, name='S·ªë l∆∞·ª£ng',\n",
        "                               marker_color='#636EFA'), row=1, col=1)\n",
        "    fig.add_trace(go.Scatter(x=sample['score'], y=sample['percentile'],\n",
        "                             mode='lines', name='Percentile', line=dict(color='#EF553B')),\n",
        "                  row=1, col=2)\n",
        "\n",
        "    fig.update_layout(height=400, title_text='üìä Ph√¢n t√≠ch Ph·ªï ƒëi·ªÉm B00 - 2025')\n",
        "    fig.show()\n",
        "\n",
        "    print(\"\\nüìã B·∫£ng tra c·ª©u m·∫´u:\")\n",
        "    for score in [27, 25, 23, 20]:\n",
        "        pct = sample[sample['score'] >= score].iloc[0]['percentile']\n",
        "        print(f\"  ƒêi·ªÉm {score} = Top {pct:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccf8437f",
      "metadata": {
        "id": "ccf8437f"
      },
      "source": [
        "## üîÑ 6. Normalize Benchmark Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc46e3b",
      "metadata": {
        "id": "8bc46e3b"
      },
      "outputs": [],
      "source": [
        "def expand_multiple_blocks(df):\n",
        "    \"\"\"T√°ch d√≤ng c√≥ nhi·ªÅu t·ªï h·ª£p\"\"\"\n",
        "    expanded_rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        to_hop = str(row['to_hop_mon']).strip()\n",
        "        blocks = [b.strip() for b in to_hop.split(',')]\n",
        "        for block in blocks:\n",
        "            if block and block in BLOCK_MAP:\n",
        "                new_row = row.copy()\n",
        "                new_row['to_hop_mon'] = block\n",
        "                expanded_rows.append(new_row)\n",
        "    return pd.DataFrame(expanded_rows)\n",
        "\n",
        "def normalize_benchmark(df, lookup_dict):\n",
        "    \"\"\"Quy ƒë·ªïi ƒëi·ªÉm chu·∫©n sang percentile\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä QUY ƒê·ªîI ƒêI·ªÇM CHU·∫®N SANG PERCENTILE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # T√°ch t·ªï h·ª£p\n",
        "    df_expanded = expand_multiple_blocks(df)\n",
        "    print(f\"Sau khi t√°ch t·ªï h·ª£p: {len(df_expanded):,} d√≤ng\")\n",
        "\n",
        "    # L·ªçc ng√†nh c√≥ 2025\n",
        "    nganh_2025 = df_expanded[df_expanded['nam'] == 2025][['university_id', 'ma_nganh', 'to_hop_mon']].drop_duplicates()\n",
        "    print(f\"Ng√†nh c√≥ data 2025: {len(nganh_2025)}\")\n",
        "\n",
        "    df_filtered = df_expanded.merge(nganh_2025, on=['university_id', 'ma_nganh', 'to_hop_mon'], how='inner')\n",
        "    print(f\"Sau khi l·ªçc: {len(df_filtered):,} d√≤ng\")\n",
        "\n",
        "    # Quy ƒë·ªïi\n",
        "    def get_percentile(row):\n",
        "        lookup = lookup_dict.get((row['nam'], str(row['to_hop_mon']).strip()))\n",
        "        if lookup is None:\n",
        "            return np.nan\n",
        "        idx = np.searchsorted(lookup['score'], row['diem_chuan'], side='left')\n",
        "        return lookup.iloc[idx]['percentile'] if idx < len(lookup) else 0.01\n",
        "\n",
        "    df_filtered['percentile_rank'] = df_filtered.apply(get_percentile, axis=1)\n",
        "    df_clean = df_filtered.dropna(subset=['percentile_rank'])\n",
        "\n",
        "    print(f\"\\n‚úÖ D·ªØ li·ªáu training: {len(df_clean):,} d√≤ng\")\n",
        "    return df_clean\n",
        "\n",
        "# Run\n",
        "df_train = normalize_benchmark(df_benchmark, lookup_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec24576",
      "metadata": {
        "id": "dec24576"
      },
      "outputs": [],
      "source": [
        "# Visualize ƒêi·ªÉm vs Percentile\n",
        "sample_data = df_train.sample(min(3000, len(df_train)))\n",
        "\n",
        "fig = px.scatter(sample_data, x='diem_chuan', y='percentile_rank',\n",
        "                 color='nam', opacity=0.6,\n",
        "                 title='üìä M·ªëi quan h·ªá ƒêi·ªÉm chu·∫©n vs Percentile',\n",
        "                 labels={'diem_chuan': 'ƒêi·ªÉm chu·∫©n', 'percentile_rank': 'Top %', 'nam': 'NƒÉm'})\n",
        "\n",
        "fig.update_layout(height=500)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c260c33",
      "metadata": {
        "id": "2c260c33"
      },
      "source": [
        "## ü§ñ 7. Advanced Models Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d03c88bc",
      "metadata": {
        "id": "d03c88bc"
      },
      "outputs": [],
      "source": [
        "# Feature Engineering\n",
        "def calculate_trend(values):\n",
        "    \"\"\"Xu h∆∞·ªõng (slope)\"\"\"\n",
        "    if len(values) < 2:\n",
        "        return 0\n",
        "    x = np.arange(len(values)).reshape(-1, 1)\n",
        "    model = LinearRegression()\n",
        "    model.fit(x, values)\n",
        "    return model.coef_[0]\n",
        "\n",
        "def calculate_volatility(values):\n",
        "    \"\"\"ƒê·ªô bi·∫øn ƒë·ªông\"\"\"\n",
        "    return np.std(values) if len(values) > 1 else 0\n",
        "\n",
        "def calculate_momentum(values):\n",
        "    \"\"\"ƒê·ªông l∆∞·ª£ng (thay ƒë·ªïi g·∫ßn ƒë√¢y)\"\"\"\n",
        "    return values[-1] - values[-2] if len(values) >= 2 else 0\n",
        "\n",
        "# Prediction Models\n",
        "def predict_weighted_average(values, alpha=2.0):\n",
        "    \"\"\"Weighted Average v·ªõi exponential weights\"\"\"\n",
        "    n = len(values)\n",
        "    if n == 0:\n",
        "        return None, None\n",
        "    weights = np.exp(np.linspace(0, alpha, n))\n",
        "    pred = np.sum(values * weights) / weights.sum()\n",
        "    weighted_var = np.sum(weights * (values - pred)**2) / weights.sum()\n",
        "    ci = 1.96 * np.sqrt(weighted_var) if weighted_var > 0 else 0\n",
        "    return pred, ci\n",
        "\n",
        "def predict_ets(values):\n",
        "    \"\"\"Exponential Smoothing\"\"\"\n",
        "    try:\n",
        "        if len(values) < 4:\n",
        "            return None, None\n",
        "        model = ExponentialSmoothing(values, trend='add', seasonal=None)\n",
        "        fit = model.fit()\n",
        "        pred = fit.forecast(1)[0]\n",
        "        residuals = values - fit.fittedvalues\n",
        "        ci = 1.96 * np.std(residuals)\n",
        "        return pred, ci\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "def predict_arima(values):\n",
        "    \"\"\"ARIMA(1,1,1)\"\"\"\n",
        "    try:\n",
        "        if len(values) < 5:\n",
        "            return None, None\n",
        "        model = ARIMA(values, order=(1, 1, 1))\n",
        "        fit = model.fit()\n",
        "        forecast = fit.get_forecast(steps=1)\n",
        "        pred = forecast.predicted_mean[0]\n",
        "        ci = forecast.conf_int().iloc[0, 1] - pred\n",
        "        return pred, ci\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "def predict_linear_regression(values):\n",
        "    \"\"\"Linear Regression\"\"\"\n",
        "    try:\n",
        "        if len(values) < 3:\n",
        "            return None, None\n",
        "        x = np.arange(len(values)).reshape(-1, 1)\n",
        "        model = LinearRegression()\n",
        "        model.fit(x, values)\n",
        "        pred = model.predict([[len(values)]])[0]\n",
        "        residuals = values - model.predict(x)\n",
        "        ci = 1.96 * np.std(residuals)\n",
        "        return pred, ci\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a 4 models: WA, ETS, ARIMA, LR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a94da41",
      "metadata": {
        "id": "9a94da41"
      },
      "outputs": [],
      "source": [
        "def ensemble_predict(values):\n",
        "    \"\"\"Ensemble prediction - K·∫øt h·ª£p nhi·ªÅu models\"\"\"\n",
        "    predictions = {}\n",
        "    confidence_intervals = {}\n",
        "\n",
        "    # Run all models\n",
        "    for name, func in [('WA', predict_weighted_average), ('ETS', predict_ets),\n",
        "                       ('ARIMA', predict_arima), ('LR', predict_linear_regression)]:\n",
        "        pred, ci = func(values)\n",
        "        if pred is not None:\n",
        "            predictions[name] = pred\n",
        "            confidence_intervals[name] = ci if ci and ci > 0 else 0.1\n",
        "\n",
        "    if not predictions:\n",
        "        return np.mean(values), np.std(values) * 1.96, 'MEAN', {}\n",
        "\n",
        "    # Weighted ensemble (weight = 1/CI)\n",
        "    total_weight = 0\n",
        "    weighted_pred = 0\n",
        "\n",
        "    for model, pred in predictions.items():\n",
        "        weight = 1 / confidence_intervals[model]\n",
        "        weighted_pred += pred * weight\n",
        "        total_weight += weight\n",
        "\n",
        "    ensemble_pred = weighted_pred / total_weight if total_weight > 0 else np.mean(list(predictions.values()))\n",
        "    best_model = min(confidence_intervals.keys(), key=lambda k: confidence_intervals[k])\n",
        "    ensemble_ci = np.mean(list(confidence_intervals.values()))\n",
        "\n",
        "    return ensemble_pred, ensemble_ci, best_model, predictions\n",
        "\n",
        "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a Ensemble Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f9d1b4",
      "metadata": {
        "id": "08f9d1b4"
      },
      "source": [
        "## üèãÔ∏è 8. Training v·ªõi Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e7c8ab",
      "metadata": {
        "id": "73e7c8ab"
      },
      "outputs": [],
      "source": [
        "def time_series_cv(values, n_splits=3):\n",
        "    \"\"\"Time Series Cross-Validation\"\"\"\n",
        "    if len(values) < n_splits + 2:\n",
        "        return None\n",
        "\n",
        "    errors = defaultdict(list)\n",
        "\n",
        "    for i in range(n_splits):\n",
        "        split_point = len(values) - n_splits + i\n",
        "        if split_point < 3:\n",
        "            continue\n",
        "\n",
        "        train = values[:split_point]\n",
        "        test = values[split_point]\n",
        "\n",
        "        for name, func in [('WA', predict_weighted_average), ('ETS', predict_ets),\n",
        "                           ('ARIMA', predict_arima), ('LR', predict_linear_regression)]:\n",
        "            pred, _ = func(train)\n",
        "            if pred is not None:\n",
        "                errors[name].append(abs(pred - test))\n",
        "\n",
        "    return {k: np.mean(v) if v else float('inf') for k, v in errors.items()}\n",
        "\n",
        "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a Time Series Cross-Validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74827f4d",
      "metadata": {
        "id": "74827f4d"
      },
      "outputs": [],
      "source": [
        "def train_advanced(df_labeled):\n",
        "    \"\"\"Advanced Training v·ªõi Ensemble & CV\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ü§ñ ADVANCED MODEL TRAINING\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    groups = df_labeled.groupby(['university_id', 'ma_nganh', 'to_hop_mon'])\n",
        "\n",
        "    forecast_results = {}\n",
        "    confidence_results = {}\n",
        "    model_selection = {}\n",
        "    feature_store = {}\n",
        "\n",
        "    stats = defaultdict(int)\n",
        "    all_errors = defaultdict(list)\n",
        "\n",
        "    total_groups = len(groups)\n",
        "    print(f\"\\nüìä Training {total_groups} nh√≥m ng√†nh...\\n\")\n",
        "\n",
        "    for i, (name, group) in enumerate(groups):\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"  ‚è≥ {i+1}/{total_groups} ({100*(i+1)/total_groups:.0f}%)\", end=\"\\r\")\n",
        "\n",
        "        group = group.sort_values('nam')\n",
        "        percentiles = group['percentile_rank'].values\n",
        "\n",
        "        # Extract features\n",
        "        features = {\n",
        "            'mean': np.mean(percentiles),\n",
        "            'std': np.std(percentiles) if len(percentiles) > 1 else 0,\n",
        "            'trend': calculate_trend(percentiles),\n",
        "            'volatility': calculate_volatility(percentiles),\n",
        "            'momentum': calculate_momentum(percentiles),\n",
        "            'n_years': len(percentiles)\n",
        "        }\n",
        "        feature_store[name] = features\n",
        "\n",
        "        # Cross-validation\n",
        "        cv_errors = time_series_cv(percentiles)\n",
        "\n",
        "        if cv_errors and len(percentiles) >= 4:\n",
        "            for model, err in cv_errors.items():\n",
        "                all_errors[model].append(err)\n",
        "\n",
        "            # Ensemble prediction\n",
        "            pred, ci, best_model, _ = ensemble_predict(percentiles)\n",
        "        else:\n",
        "            pred, ci = predict_weighted_average(percentiles)\n",
        "            if pred is None:\n",
        "                pred = np.mean(percentiles)\n",
        "                ci = np.std(percentiles) * 1.96\n",
        "            best_model = 'WA'\n",
        "            stats['WA_only'] += 1\n",
        "\n",
        "        # Safety factor\n",
        "        adjusted_pred = pred * 0.95\n",
        "\n",
        "        forecast_results[name] = adjusted_pred\n",
        "        confidence_results[name] = ci if ci else 0\n",
        "        model_selection[name] = best_model\n",
        "        stats[best_model] += 1\n",
        "\n",
        "    print(f\"\\n\\n‚úÖ ƒê√£ train {len(forecast_results)} nh√≥m!\")\n",
        "    print(f\"\\nüìä Model Selection:\")\n",
        "    for model, count in sorted(stats.items()):\n",
        "        print(f\"   {model}: {count} ({100*count/total_groups:.1f}%)\")\n",
        "\n",
        "    if all_errors:\n",
        "        print(f\"\\nüìâ CV Error (MAE):\")\n",
        "        for model, errors in all_errors.items():\n",
        "            if errors:\n",
        "                print(f\"   {model}: {np.mean(errors):.3f} ¬± {np.std(errors):.3f}\")\n",
        "\n",
        "    analytics = {\n",
        "        'predictions': forecast_results,\n",
        "        'confidence_intervals': confidence_results,\n",
        "        'model_selection': model_selection,\n",
        "        'features': feature_store,\n",
        "        'statistics': dict(stats),\n",
        "        'cv_errors': {k: float(np.mean(v)) for k, v in all_errors.items() if v}\n",
        "    }\n",
        "\n",
        "    return forecast_results, confidence_results, analytics\n",
        "\n",
        "# RUN TRAINING\n",
        "model_2026, confidence_2026, analytics = train_advanced(df_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df62744c",
      "metadata": {
        "id": "df62744c"
      },
      "source": [
        "## üìä 9. Model Evaluation & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00fc7a7c",
      "metadata": {
        "id": "00fc7a7c"
      },
      "outputs": [],
      "source": [
        "# 9.1 Model Selection Pie Chart\n",
        "model_counts = analytics['statistics']\n",
        "models = [k for k in model_counts.keys() if not k.endswith('_only')]\n",
        "counts = [model_counts[k] for k in models]\n",
        "\n",
        "fig = go.Figure(data=[go.Pie(\n",
        "    labels=models,\n",
        "    values=counts,\n",
        "    hole=0.4,\n",
        "    marker_colors=['#636EFA', '#EF553B', '#00CC96', '#AB63FA', '#FFA15A']\n",
        ")])\n",
        "\n",
        "fig.update_layout(\n",
        "    title='üèÜ Model Selection Distribution',\n",
        "    height=400\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e58f194",
      "metadata": {
        "id": "9e58f194"
      },
      "outputs": [],
      "source": [
        "# 9.2 CV Error Comparison\n",
        "cv_errors = analytics.get('cv_errors', {})\n",
        "\n",
        "if cv_errors:\n",
        "    fig = go.Figure(go.Bar(\n",
        "        x=list(cv_errors.values()),\n",
        "        y=list(cv_errors.keys()),\n",
        "        orientation='h',\n",
        "        marker=dict(color=['#636EFA', '#EF553B', '#00CC96', '#AB63FA'][:len(cv_errors)])\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title='üìâ Cross-Validation Error (MAE)',\n",
        "        xaxis_title='Mean Absolute Error',\n",
        "        height=400\n",
        "    )\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e093f8d8",
      "metadata": {
        "id": "e093f8d8"
      },
      "outputs": [],
      "source": [
        "# 9.3 Confidence Interval Distribution\n",
        "ci_values = list(confidence_2026.values())\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Histogram(x=ci_values, nbinsx=40, marker_color='#636EFA', opacity=0.7))\n",
        "fig.add_vline(x=np.mean(ci_values), line_dash=\"dash\", line_color=\"red\",\n",
        "              annotation_text=f\"Mean: {np.mean(ci_values):.2f}\")\n",
        "\n",
        "fig.update_layout(\n",
        "    title='üìä Confidence Interval Distribution',\n",
        "    xaxis_title='Confidence Interval (percentile points)',\n",
        "    yaxis_title='Frequency',\n",
        "    height=400\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f76599c",
      "metadata": {
        "id": "7f76599c"
      },
      "outputs": [],
      "source": [
        "# 9.4 Prediction Distribution\n",
        "pred_values = list(model_2026.values())\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Histogram(x=pred_values, nbinsx=40, marker_color='#00CC96', opacity=0.7))\n",
        "fig.add_vline(x=np.mean(pred_values), line_dash=\"dash\", line_color=\"red\",\n",
        "              annotation_text=f\"Mean: {np.mean(pred_values):.2f}\")\n",
        "\n",
        "fig.update_layout(\n",
        "    title='üéØ Predicted Percentile Distribution (2026)',\n",
        "    xaxis_title='Predicted Percentile (Top %)',\n",
        "    yaxis_title='Frequency',\n",
        "    height=400\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f06c9c",
      "metadata": {
        "id": "95f06c9c"
      },
      "outputs": [],
      "source": [
        "# 9.5 Feature Importance - Trend vs Volatility\n",
        "features_df = pd.DataFrame([\n",
        "    {'key': k, 'trend': v['trend'], 'volatility': v['volatility'], 'n_years': v['n_years']}\n",
        "    for k, v in analytics['features'].items()\n",
        "])\n",
        "\n",
        "fig = px.scatter(features_df.sample(min(500, len(features_df))),\n",
        "                 x='trend', y='volatility', size='n_years',\n",
        "                 title='üìà Feature Analysis: Trend vs Volatility',\n",
        "                 labels={'trend': 'Trend (Xu h∆∞·ªõng)', 'volatility': 'Volatility (Bi·∫øn ƒë·ªông)'},\n",
        "                 color='n_years', color_continuous_scale='Viridis')\n",
        "\n",
        "fig.update_layout(height=500)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c6e14e4",
      "metadata": {
        "id": "9c6e14e4"
      },
      "source": [
        "## üíæ 10. Save & Download Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d867a94",
      "metadata": {
        "id": "9d867a94"
      },
      "outputs": [],
      "source": [
        "!mkdir -p model_artifacts\n",
        "\n",
        "# Save\n",
        "joblib.dump(model_2026, 'model_artifacts/university_ranking_model_2026.pkl')\n",
        "\n",
        "lookup_2025 = {k: v for k, v in lookup_map.items() if k[0] == 2025}\n",
        "joblib.dump(lookup_2025, 'model_artifacts/score_distribution_2025.pkl')\n",
        "\n",
        "joblib.dump(analytics, 'model_artifacts/model_analytics.pkl')\n",
        "\n",
        "# Save confidence intervals\n",
        "joblib.dump(confidence_2026, 'model_artifacts/confidence_intervals.pkl')\n",
        "\n",
        "print(\"‚úÖ ƒê√£ l∆∞u t·∫•t c·∫£ files!\")\n",
        "print(f\"   - Model: {len(model_2026)} ng√†nh\")\n",
        "print(f\"   - Lookup: {len(lookup_2025)} t·ªï h·ª£p\")\n",
        "print(f\"   - Analytics: Full statistics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fed05519",
      "metadata": {
        "id": "fed05519"
      },
      "outputs": [],
      "source": [
        "# L∆∞u c√°c files c·∫ßn thi·∫øt\n",
        "print(\"üíæ ƒê√£ l∆∞u t·∫•t c·∫£ models v√†o th∆∞ m·ª•c model_artifacts/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a594d86",
      "metadata": {
        "id": "5a594d86"
      },
      "source": [
        "## üéØ 11. Demo: T√¨m Ng√†nh Ph√π H·ª£p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c8afc6f",
      "metadata": {
        "id": "1c8afc6f"
      },
      "outputs": [],
      "source": [
        "def get_user_percentile(score, block, lookup_dict):\n",
        "    \"\"\"Quy ƒë·ªïi ƒëi·ªÉm user sang percentile\"\"\"\n",
        "    key = (2025, block)\n",
        "    if key not in lookup_dict:\n",
        "        return None\n",
        "    table = lookup_dict[key]\n",
        "    idx = np.searchsorted(table['score'], score, side='left')\n",
        "    return table.iloc[idx]['percentile'] if idx < len(table) else 0.01\n",
        "\n",
        "def find_suitable_majors(student_scores, model, confidence, lookup, df_info, priority=0, top_n=30):\n",
        "    \"\"\"T√¨m ng√†nh ph√π h·ª£p v·ªõi confidence interval\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for key, predicted_percentile in model.items():\n",
        "        university_id, ma_nganh, block = key\n",
        "\n",
        "        if block not in BLOCK_MAP:\n",
        "            continue\n",
        "\n",
        "        subjects = BLOCK_MAP[block]\n",
        "        if not all(s in student_scores for s in subjects):\n",
        "            continue\n",
        "\n",
        "        diem_cua_ban = sum(student_scores[s] for s in subjects) + priority\n",
        "        student_percentile = get_user_percentile(diem_cua_ban, block, lookup)\n",
        "\n",
        "        if student_percentile is None:\n",
        "            continue\n",
        "\n",
        "        khoang_cach = predicted_percentile - student_percentile\n",
        "        ci = confidence.get(key, 0)\n",
        "\n",
        "        # L·∫•y info\n",
        "        info = df_info[(df_info['university_id'] == university_id) & (df_info['ma_nganh'] == ma_nganh)]\n",
        "        if info.empty:\n",
        "            continue\n",
        "        info = info.iloc[0]\n",
        "\n",
        "        # ƒê√°nh gi√°\n",
        "        if khoang_cach > 10:\n",
        "            kha_nang = \"üü¢ R·∫•t cao\"\n",
        "            score = 5\n",
        "        elif khoang_cach > 2:\n",
        "            kha_nang = \"üü¢ Cao\"\n",
        "            score = 4\n",
        "        elif khoang_cach > 0:\n",
        "            kha_nang = \"üü° Trung b√¨nh\"\n",
        "            score = 3\n",
        "        elif khoang_cach > -2:\n",
        "            kha_nang = \"üü† Th·∫•p\"\n",
        "            score = 2\n",
        "        else:\n",
        "            kha_nang = \"üî¥ R·∫•t th·∫•p\"\n",
        "            score = 1\n",
        "\n",
        "        results.append({\n",
        "            'Tr∆∞·ªùng': info['ten_truong'],\n",
        "            'Ng√†nh': info['ten_nganh'],\n",
        "            'T·ªï h·ª£p': block,\n",
        "            'ƒêi·ªÉm b·∫°n': diem_cua_ban,\n",
        "            'Kh·∫£ nƒÉng': kha_nang,\n",
        "            'CI ¬±': round(ci, 1),\n",
        "            'score': score\n",
        "        })\n",
        "\n",
        "    df_result = pd.DataFrame(results).sort_values('score', ascending=False)\n",
        "    return df_result.head(top_n)\n",
        "\n",
        "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a h√†m t√¨m ng√†nh!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9f108f5",
      "metadata": {
        "id": "b9f108f5"
      },
      "outputs": [],
      "source": [
        "# === NH·∫¨P ƒêI·ªÇM C·ª¶A B·∫†N ===\n",
        "print(\"=\"*50)\n",
        "print(\"üéì DEMO: T√åM NG√ÄNH PH√ô H·ª¢P\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "student_scores = {\n",
        "    'toan': 8.5,\n",
        "    'vat_ly': 8.0,\n",
        "    'hoa_hoc': 8.5,\n",
        "    'sinh_hoc': 7.5,\n",
        "    'ngu_van': 7.0,\n",
        "    'ngoai_ngu': 8.0\n",
        "}\n",
        "priority = 0\n",
        "\n",
        "print(\"\\nüìù ƒêi·ªÉm c·ªßa b·∫°n:\")\n",
        "for s, score in student_scores.items():\n",
        "    print(f\"   {s}: {score}\")\n",
        "\n",
        "print(\"\\nüìä ƒêi·ªÉm t·ªï h·ª£p:\")\n",
        "for block, subjects in BLOCK_MAP.items():\n",
        "    total = sum(student_scores.get(s, 0) for s in subjects) + priority\n",
        "    print(f\"   {block}: {total:.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0522ce9",
      "metadata": {
        "id": "b0522ce9"
      },
      "outputs": [],
      "source": [
        "# T√¨m ng√†nh\n",
        "suitable = find_suitable_majors(student_scores, model_2026, confidence_2026, lookup_2025, df_benchmark, priority)\n",
        "\n",
        "if not suitable.empty:\n",
        "    print(\"\\nüéØ NG√ÄNH PH√ô H·ª¢P:\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    display(suitable[['Tr∆∞·ªùng', 'Ng√†nh', 'T·ªï h·ª£p', 'ƒêi·ªÉm b·∫°n', 'Kh·∫£ nƒÉng', 'CI ¬±']])\n",
        "\n",
        "    # Pie chart\n",
        "    chance_counts = suitable['Kh·∫£ nƒÉng'].value_counts()\n",
        "    fig = px.pie(values=chance_counts.values, names=chance_counts.index,\n",
        "                 title='üìä Ph√¢n b·ªë C∆° h·ªôi ƒë·∫≠u', hole=0.4)\n",
        "    fig.show()\n",
        "else:\n",
        "    print(\"‚ùå Kh√¥ng t√¨m th·∫•y ng√†nh ph√π h·ª£p\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b40ff875",
      "metadata": {
        "id": "b40ff875"
      },
      "source": [
        "## üìä K·∫æT LU·∫¨N PH·∫¶N 1: Percentile Prediction\n",
        "\n",
        "Ph·∫ßn tr√™n ƒë√£ ho√†n th√†nh **M·ª•c ti√™u 1**: D·ª± ƒëo√°n percentile ƒëi·ªÉm chu·∫©n 2026\n",
        "\n",
        "---\n",
        "\n",
        "# üéØ PH·∫¶N 2: ADMISSION PROBABILITY MODEL (v3 Approach)\n",
        "\n",
        "## T·∫°i sao c·∫ßn ph·∫ßn n√†y?\n",
        "\n",
        "**Ph·∫ßn 1** cho bi·∫øt: \"Ng√†nh Y khoa s·∫Ω l·∫•y Top 0.5%\"  \n",
        "**Ph·∫ßn 2** cho bi·∫øt: \"V·ªõi ƒëi·ªÉm 27, b·∫°n c√≥ 65% c∆° h·ªôi ƒë·∫≠u Y khoa\"\n",
        "\n",
        "### üí° Key Insight - T·∫°i sao v3 kh√°c v1/v2?\n",
        "\n",
        "```\n",
        "‚ùå v1/v2 (SAI):\n",
        "   Features: [ƒëi·ªÉm_thi, gap, percentile, ...]\n",
        "   gap = ƒëi·ªÉm_thi - ƒëi·ªÉm_chu·∫©n_nƒÉm_ƒë√≥\n",
        "   \n",
        "   V·∫•n ƒë·ªÅ: Khi predict 2026, ta KH√îNG C√ì ƒëi·ªÉm_chu·∫©n_2026!\n",
        "\n",
        "‚úÖ v3 (ƒê√öNG):\n",
        "   Features: [student_percentile, percentile_required, university, major, ...]\n",
        "   \n",
        "   C√°ch ho·∫°t ƒë·ªông: Model h·ªçc pattern t·ª´ l·ªãch s·ª≠\n",
        "   \"Ng√†nh Y khoa th∆∞·ªùng y√™u c·∫ßu Top 0.5%\"\n",
        "   \"Th√≠ sinh Top 0.3% c√≥ 80% c∆° h·ªôi ƒë·∫≠u ng√†nh y√™u c·∫ßu Top 0.5%\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc4cf76",
      "metadata": {
        "id": "2fc4cf76"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# B∆Ø·ªöC 1: CHU·∫®N B·ªä D·ªÆ LI·ªÜU CHO ADMISSION PROBABILITY\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üéØ ADMISSION PROBABILITY MODEL v3 - REALISTIC APPROACH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load l·∫°i d·ªØ li·ªáu ƒëi·ªÉm thi (n·∫øu ch∆∞a c√≥)\n",
        "all_score_data = {}  # {(year, block): scores array}\n",
        "\n",
        "print(\"\\n[STEP 1] Loading ƒëi·ªÉm thi THPT (2020-2025)...\")\n",
        "for f in sorted(glob.glob(os.path.join(SCORE_FOLDER, '*.csv'))):\n",
        "    year = int(re.search(r'\\d{4}', os.path.basename(f)).group())\n",
        "    if year < 2020:\n",
        "        continue\n",
        "\n",
        "    print(f\"   -> NƒÉm {year}...\", end=\" \")\n",
        "    df_scores = pd.read_csv(f)\n",
        "\n",
        "    for block, cols in BLOCK_MAP.items():\n",
        "        if all(c in df_scores.columns for c in cols):\n",
        "            temp = df_scores.dropna(subset=cols)\n",
        "            scores = temp[cols].sum(axis=1).values\n",
        "            all_score_data[(year, block)] = scores\n",
        "    print(f\"‚úÖ ({len(df_scores):,} th√≠ sinh)\")\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded {len(all_score_data)} c·∫∑p (nƒÉm, t·ªï h·ª£p)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04d28af8",
      "metadata": {
        "id": "04d28af8"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# B∆Ø·ªöC 2: T√çNH PERCENTILE CHO ƒêI·ªÇM CHU·∫®N\n",
        "# =====================================================\n",
        "# Key insight: M·ªói ng√†nh \"y√™u c·∫ßu\" m·ªôt percentile nh·∫•t ƒë·ªãnh\n",
        "# V√≠ d·ª•: Y khoa c·∫ßn Top 0.5%, D∆∞·ª£c c·∫ßn Top 2%\n",
        "\n",
        "print(\"\\n[STEP 2] T√≠nh percentile_required cho m·ªói ng√†nh...\")\n",
        "\n",
        "def get_percentile_for_score(score, year, block):\n",
        "    \"\"\"T√≠nh percentile c·ªßa ƒëi·ªÉm trong ph√¢n ph·ªëi nƒÉm ƒë√≥\"\"\"\n",
        "    key = (year, block)\n",
        "    if key not in all_score_data:\n",
        "        return None\n",
        "    scores = all_score_data[key]\n",
        "    # Percentile = % th√≠ sinh c√≥ ƒëi·ªÉm <= score\n",
        "    return (scores <= score).sum() / len(scores) * 100\n",
        "\n",
        "# Load ƒëi·ªÉm chu·∫©n\n",
        "df_benchmark_v3 = pd.read_csv(BENCHMARK_FILE)\n",
        "df_benchmark_v3 = df_benchmark_v3[df_benchmark_v3['nam'] >= 2020]\n",
        "\n",
        "# Th√™m c·ªôt percentile_required\n",
        "df_benchmark_v3['percentile_required'] = df_benchmark_v3.apply(\n",
        "    lambda row: get_percentile_for_score(row['diem_chuan'], row['nam'], row['to_hop_mon']),\n",
        "    axis=1\n",
        ")\n",
        "df_benchmark_v3 = df_benchmark_v3.dropna(subset=['percentile_required'])\n",
        "\n",
        "print(f\"   -> {len(df_benchmark_v3)} ng√†nh c√≥ percentile_required\")\n",
        "\n",
        "# Hi·ªÉn th·ªã v√≠ d·ª•\n",
        "print(\"\\nüìã V√≠ d·ª• percentile_required:\")\n",
        "sample = df_benchmark_v3[['ten_truong', 'ten_nganh', 'to_hop_mon', 'diem_chuan', 'percentile_required', 'nam']].head(5)\n",
        "print(sample.to_string(index=False))\n",
        "\n",
        "# =====================================================\n",
        "# B∆Ø·ªöC 3: T·∫†O TRAINING DATA V·ªöI STRATIFIED SAMPLING\n",
        "# =====================================================\n",
        "# C√°ch ti·∫øp c·∫≠n v3 + Stratified Sampling:\n",
        "# - Sample ƒëi·ªÉm thi th·∫≠t t·ª´ ph√¢n ph·ªëi theo ZONES\n",
        "# - T√≠nh student_percentile (v·ªã tr√≠ c·ªßa th√≠ sinh)\n",
        "# - Label: 1 n·∫øu student_percentile >= percentile_required, 0 n·∫øu kh√¥ng\n",
        "\n",
        "print(\"\\n[STEP 3] T·∫°o training data v·ªõi Stratified Sampling...\")\n",
        "\n",
        "def stratified_sample(scores, diem_chuan, total_samples=300):\n",
        "    \"\"\"\n",
        "    Stratified Sampling: L·∫•y m·∫´u th√¥ng minh theo 4 zones\n",
        "    - Zone 1 (r·ªõt ch·∫Øc): gap < -5     -> 15% samples\n",
        "    - Zone 2 (c√≥ th·ªÉ r·ªõt): -5 <= gap < -1  -> 25% samples\n",
        "    - Zone 3 (ranh gi·ªõi): -1 <= gap < +3   -> 40% samples\n",
        "    - Zone 4 (ƒë·∫≠u ch·∫Øc): gap >= +3    -> 20% samples\n",
        "\n",
        "    ∆Øu ƒëi·ªÉm:\n",
        "    - Gi·ªØ l·∫°i extreme cases (model v·∫´n h·ªçc ƒëi·ªÉm cao=ƒë·∫≠u, th·∫•p=r·ªõt)\n",
        "    - T·∫≠p trung v√†o v√πng ranh gi·ªõi (n∆°i model c·∫ßn h·ªçc ph√¢n bi·ªát)\n",
        "    - Model generalize t·ªët h∆°n khi deploy\n",
        "    \"\"\"\n",
        "    gaps = scores - diem_chuan\n",
        "\n",
        "    # Ph√¢n chia zones\n",
        "    zone1_mask = gaps < -5\n",
        "    zone2_mask = (gaps >= -5) & (gaps < -1)\n",
        "    zone3_mask = (gaps >= -1) & (gaps < 3)\n",
        "    zone4_mask = gaps >= 3\n",
        "\n",
        "    zone_scores = [\n",
        "        scores[zone1_mask],\n",
        "        scores[zone2_mask],\n",
        "        scores[zone3_mask],\n",
        "        scores[zone4_mask]\n",
        "    ]\n",
        "\n",
        "    # T·ª∑ l·ªá samples cho m·ªói zone\n",
        "    zone_ratios = [0.15, 0.25, 0.40, 0.20]\n",
        "\n",
        "    sampled_all = []\n",
        "    for zone_data, ratio in zip(zone_scores, zone_ratios):\n",
        "        n_want = int(total_samples * ratio)\n",
        "        if len(zone_data) > 0:\n",
        "            n_take = min(n_want, len(zone_data))\n",
        "            sampled = np.random.choice(zone_data, size=n_take, replace=False)\n",
        "            sampled_all.extend(sampled)\n",
        "\n",
        "    return np.array(sampled_all)\n",
        "\n",
        "training_data = []\n",
        "sample_per_major = 300  # S·ªë samples m·ªói ng√†nh\n",
        "\n",
        "for _, row in df_benchmark_v3.iterrows():\n",
        "    block = str(row['to_hop_mon']).strip()\n",
        "    nam = row['nam']\n",
        "    key = (nam, block)\n",
        "\n",
        "    if key not in all_score_data:\n",
        "        continue\n",
        "\n",
        "    scores = all_score_data[key]\n",
        "    diem_chuan = row['diem_chuan']\n",
        "    percentile_required = row['percentile_required']\n",
        "    university_id = row['university_id']\n",
        "    ma_nganh = row['ma_nganh']\n",
        "\n",
        "    # STRATIFIED SAMPLING thay v√¨ random sampling\n",
        "    sampled = stratified_sample(scores, diem_chuan, sample_per_major)\n",
        "\n",
        "    for score in sampled:\n",
        "        # T√≠nh student_percentile\n",
        "        student_percentile = (scores <= score).sum() / len(scores) * 100\n",
        "\n",
        "        # Label: ƒë·∫≠u n·∫øu percentile >= percentile_required\n",
        "        # (percentile cao = top ƒë·∫ßu, percentile_required th·∫•p = ng√†nh kh√≥)\n",
        "        label = 1 if student_percentile >= percentile_required else 0\n",
        "\n",
        "        training_data.append({\n",
        "            'university_id': university_id,\n",
        "            'ma_nganh': ma_nganh,\n",
        "            'to_hop_mon': block,\n",
        "            'student_percentile': student_percentile,\n",
        "            'percentile_required': percentile_required,\n",
        "            'label': label\n",
        "        })\n",
        "\n",
        "df_train_prob = pd.DataFrame(training_data)\n",
        "print(f\"\\n‚úÖ T·∫°o ƒë∆∞·ª£c {len(df_train_prob):,} training samples\")\n",
        "print(f\"   -> ƒê·∫≠u: {(df_train_prob['label']==1).sum():,} ({(df_train_prob['label']==1).mean()*100:.1f}%)\")\n",
        "print(f\"   -> Tr∆∞·ª£t: {(df_train_prob['label']==0).sum():,} ({(df_train_prob['label']==0).mean()*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e358229",
      "metadata": {
        "id": "4e358229"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# B∆Ø·ªöC 3: T·∫†O TRAINING DATA V·ªöI STRATIFIED SAMPLING\n",
        "# =====================================================\n",
        "# C√°ch ti·∫øp c·∫≠n v3 + Stratified Sampling:\n",
        "# - Sample ƒëi·ªÉm thi th·∫≠t t·ª´ ph√¢n ph·ªëi theo ZONES\n",
        "# - T√≠nh student_percentile (v·ªã tr√≠ c·ªßa th√≠ sinh)\n",
        "# - Label: 1 n·∫øu student_percentile >= percentile_required, 0 n·∫øu kh√¥ng\n",
        "\n",
        "print(\"\\n[STEP 3] T·∫°o training data v·ªõi Stratified Sampling...\")\n",
        "\n",
        "def stratified_sample(scores, diem_chuan, total_samples=300):\n",
        "    \"\"\"\n",
        "    Stratified Sampling: L·∫•y m·∫´u th√¥ng minh theo 4 zones\n",
        "    - Zone 1 (r·ªõt ch·∫Øc): gap < -5     -> 15% samples\n",
        "    - Zone 2 (c√≥ th·ªÉ r·ªõt): -5 <= gap < -1  -> 25% samples\n",
        "    - Zone 3 (ranh gi·ªõi): -1 <= gap < +3   -> 40% samples\n",
        "    - Zone 4 (ƒë·∫≠u ch·∫Øc): gap >= +3    -> 20% samples\n",
        "\n",
        "    ∆Øu ƒëi·ªÉm:\n",
        "    - Gi·ªØ l·∫°i extreme cases (model v·∫´n h·ªçc ƒëi·ªÉm cao=ƒë·∫≠u, th·∫•p=r·ªõt)\n",
        "    - T·∫≠p trung v√†o v√πng ranh gi·ªõi (n∆°i model c·∫ßn h·ªçc ph√¢n bi·ªát)\n",
        "    - Model generalize t·ªët h∆°n khi deploy\n",
        "    \"\"\"\n",
        "    gaps = scores - diem_chuan\n",
        "\n",
        "    # Ph√¢n chia zones\n",
        "    zone1_mask = gaps < -5\n",
        "    zone2_mask = (gaps >= -5) & (gaps < -1)\n",
        "    zone3_mask = (gaps >= -1) & (gaps < 3)\n",
        "    zone4_mask = gaps >= 3\n",
        "\n",
        "    zone_scores = [\n",
        "        scores[zone1_mask],\n",
        "        scores[zone2_mask],\n",
        "        scores[zone3_mask],\n",
        "        scores[zone4_mask]\n",
        "    ]\n",
        "\n",
        "    # T·ª∑ l·ªá samples cho m·ªói zone\n",
        "    zone_ratios = [0.15, 0.25, 0.40, 0.20]\n",
        "\n",
        "    sampled_all = []\n",
        "    for zone_data, ratio in zip(zone_scores, zone_ratios):\n",
        "        n_want = int(total_samples * ratio)\n",
        "        if len(zone_data) > 0:\n",
        "            n_take = min(n_want, len(zone_data))\n",
        "            sampled = np.random.choice(zone_data, size=n_take, replace=False)\n",
        "            sampled_all.extend(sampled)\n",
        "\n",
        "    return np.array(sampled_all)\n",
        "\n",
        "training_data = []\n",
        "sample_per_major = 300  # S·ªë samples m·ªói ng√†nh\n",
        "\n",
        "for _, row in df_benchmark_v3.iterrows():\n",
        "    block = str(row['to_hop_mon']).strip()\n",
        "    nam = row['nam']\n",
        "    key = (nam, block)\n",
        "\n",
        "    if key not in all_score_data:\n",
        "        continue\n",
        "\n",
        "    scores = all_score_data[key]\n",
        "    diem_chuan = row['diem_chuan']\n",
        "    percentile_required = row['percentile_required']\n",
        "    university_id = row['university_id']\n",
        "    ma_nganh = row['ma_nganh']\n",
        "\n",
        "    # STRATIFIED SAMPLING thay v√¨ random sampling\n",
        "    sampled = stratified_sample(scores, diem_chuan, sample_per_major)\n",
        "\n",
        "    for score in sampled:\n",
        "        # T√≠nh student_percentile\n",
        "        student_percentile = (scores <= score).sum() / len(scores) * 100\n",
        "\n",
        "        # Label: ƒë·∫≠u n·∫øu percentile >= percentile_required\n",
        "        # (percentile cao = top ƒë·∫ßu, percentile_required th·∫•p = ng√†nh kh√≥)\n",
        "        label = 1 if student_percentile >= percentile_required else 0\n",
        "\n",
        "        training_data.append({\n",
        "            'university_id': university_id,\n",
        "            'ma_nganh': ma_nganh,\n",
        "            'to_hop_mon': block,\n",
        "            'student_percentile': student_percentile,\n",
        "            'percentile_required': percentile_required,\n",
        "            'label': label\n",
        "        })\n",
        "\n",
        "df_train_prob = pd.DataFrame(training_data)\n",
        "print(f\"\\n‚úÖ T·∫°o ƒë∆∞·ª£c {len(df_train_prob):,} training samples\")\n",
        "print(f\"   -> ƒê·∫≠u: {(df_train_prob['label']==1).sum():,} ({(df_train_prob['label']==1).mean()*100:.1f}%)\")\n",
        "print(f\"   -> Tr∆∞·ª£t: {(df_train_prob['label']==0).sum():,} ({(df_train_prob['label']==0).mean()*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b519a757",
      "metadata": {
        "id": "b519a757"
      },
      "source": [
        "### üí° Gi·∫£i th√≠ch Logic Training Data\n",
        "\n",
        "```\n",
        "Ng√†nh Y khoa H√† N·ªôi y√™u c·∫ßu Top 0.5% (percentile_required = 99.5)\n",
        "\n",
        "Th√≠ sinh A: ƒêi·ªÉm 27.5 = Top 0.3% (student_percentile = 99.7)\n",
        "‚Üí 99.7 >= 99.5 ‚Üí Label = 1 (ƒê·∫≠u ‚úÖ)\n",
        "\n",
        "Th√≠ sinh B: ƒêi·ªÉm 25.0 = Top 2% (student_percentile = 98.0)  \n",
        "‚Üí 98.0 < 99.5 ‚Üí Label = 0 (Tr∆∞·ª£t ‚ùå)\n",
        "```\n",
        "\n",
        "**T·∫°i sao c√°ch n√†y realistic?**\n",
        "- Kh√¥ng c·∫ßn bi·∫øt ƒëi·ªÉm chu·∫©n 2026\n",
        "- Model h·ªçc pattern: \"Ng√†nh n√†o th∆∞·ªùng l·∫•y Top bao nhi√™u %\"\n",
        "- C√≥ th·ªÉ predict: \"V·ªõi v·ªã tr√≠ Top X% c·ªßa b·∫°n, kh·∫£ nƒÉng ƒë·∫≠u ng√†nh Y l√† Z%\"\n",
        "\n",
        "### üí° Gi·∫£i th√≠ch Stratified Sampling\n",
        "\n",
        "```\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "                    PH√ÇN B·ªê ƒêI·ªÇM                       \n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "   Zone 1    ‚îÇ   Zone 2    ‚îÇ   Zone 3    ‚îÇ  Zone 4  \n",
        "  R·ªõt ch·∫Øc   ‚îÇ  C√≥ th·ªÉ r·ªõt ‚îÇ  Ranh gi·ªõi  ‚îÇ ƒê·∫≠u ch·∫Øc\n",
        "   (<-5)     ‚îÇ   (-5‚Üí-1)   ‚îÇ   (-1‚Üí+3)   ‚îÇ  (>+3)   \n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "  Sample 15% ‚îÇ  Sample 25% ‚îÇ  Sample 40% ‚îÇSample 20%\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "```\n",
        "\n",
        "**T·∫°i sao Stratified Sampling t·ªët h∆°n Random Sampling?**\n",
        "\n",
        "| Gap (ƒëi·ªÉm - ƒêC) | Random Sampling | Stratified Sampling |\n",
        "|-----------------|-----------------|---------------------|\n",
        "| +10 (ƒë·∫≠u ch·∫Øc)  | üü† √çt samples    | ‚úÖ Gi·ªØ 20%           |\n",
        "| +2 (s√°t m√©p)     | üü† Random       | ‚úÖ Gi·ªØ 40%           |\n",
        "| -2 (s√°t m√©p)     | üü† Random       | ‚úÖ Gi·ªØ 40%           |\n",
        "| -10 (r·ªõt ch·∫Øc)  | üü† Nhi·ªÅu samples | ‚úÖ Gi·ªØ 15%           |\n",
        "\n",
        "**∆Øu ƒëi·ªÉm:**\n",
        "1. **Gi·ªØ l·∫°i extreme cases**: Model v·∫´n h·ªçc \"ƒëi·ªÉm cao = ƒë·∫≠u\", \"ƒëi·ªÉm th·∫•p = r·ªõt\"\n",
        "2. **T·∫≠p trung v√πng quan tr·ªçng**: Zone 2 & 3 (v√πng ranh gi·ªõi) ƒë∆∞·ª£c sample nhi·ªÅu h∆°n\n",
        "3. **Robust khi deploy**: Model x·ª≠ l√Ω ƒë∆∞·ª£c m·ªçi input, kh√¥ng b·ªã \"out of distribution\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d8a2c24",
      "metadata": {
        "id": "5d8a2c24"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# B∆Ø·ªöC 4: FEATURE ENGINEERING\n",
        "# =====================================================\n",
        "\n",
        "print(\"\\n[STEP 4] Feature Engineering...\")\n",
        "\n",
        "le_university = LabelEncoder()\n",
        "le_nganh = LabelEncoder()\n",
        "le_block = LabelEncoder()\n",
        "\n",
        "df_train_prob['uni_enc'] = le_university.fit_transform(df_train_prob['university_id'].astype(str))\n",
        "df_train_prob['nganh_enc'] = le_nganh.fit_transform(df_train_prob['ma_nganh'].astype(str))\n",
        "df_train_prob['block_enc'] = le_block.fit_transform(df_train_prob['to_hop_mon'])\n",
        "\n",
        "# QUAN TR·ªåNG: Features KH√îNG bao g·ªìm ƒëi·ªÉm chu·∫©n hay gap!\n",
        "feature_cols = [\n",
        "    'student_percentile',    # V·ªã tr√≠ c·ªßa th√≠ sinh (Top bao nhi√™u %)\n",
        "    'percentile_required',   # Ng√†nh y√™u c·∫ßu Top bao nhi√™u % (t·ª´ l·ªãch s·ª≠)\n",
        "    'uni_enc',               # Encoded tr∆∞·ªùng\n",
        "    'nganh_enc',             # Encoded ng√†nh\n",
        "    'block_enc',             # Encoded t·ªï h·ª£p\n",
        "]\n",
        "\n",
        "X = df_train_prob[feature_cols].values\n",
        "y = df_train_prob['label'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(f\"   ‚úÖ Features: {feature_cols}\")\n",
        "print(f\"   ‚úÖ X shape: {X_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c23d19",
      "metadata": {
        "id": "a4c23d19"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# B∆Ø·ªöC 5: TRAIN CLASSIFICATION MODELS\n",
        "# =====================================================\n",
        "\n",
        "print(\"\\n[STEP 5] Training Classification Models...\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"   Train: {len(X_train):,}, Test: {len(X_test):,}\")\n",
        "\n",
        "# Train Gradient Boosting (t·ªët cho b√†i to√°n n√†y)\n",
        "print(\"\\n   Training Gradient Boosting...\")\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = gb_model.predict(X_test)\n",
        "y_prob = gb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(f\"\\nüìä RESULTS:\")\n",
        "print(f\"   Accuracy: {accuracy:.4f}\")\n",
        "print(f\"   ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "# Cross validation\n",
        "cv_scores = cross_val_score(gb_model, X_scaled, y, cv=5, scoring='roc_auc')\n",
        "print(f\"   CV Score: {cv_scores.mean():.4f} (¬±{cv_scores.std()*2:.4f})\")\n",
        "\n",
        "# Feature importance\n",
        "print(f\"\\nüìà Feature Importance:\")\n",
        "for i, col in enumerate(feature_cols):\n",
        "    print(f\"   {col}: {gb_model.feature_importances_[i]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09005c3f",
      "metadata": {
        "id": "09005c3f"
      },
      "outputs": [],
      "source": [
        "# Visualize Feature Importance\n",
        "fig = go.Figure(go.Bar(\n",
        "    x=gb_model.feature_importances_,\n",
        "    y=feature_cols,\n",
        "    orientation='h',\n",
        "    marker=dict(color=gb_model.feature_importances_, colorscale='Blues')\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='üìä Feature Importance - Admission Probability Model',\n",
        "    xaxis_title='Importance',\n",
        "    height=400\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e6e1615",
      "metadata": {
        "id": "5e6e1615"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# B∆Ø·ªöC 6: S·ª¨ D·ª§NG ENSEMBLE MODEL 2026 (thay v√¨ trung b√¨nh l·ªãch s·ª≠)\n",
        "# =====================================================\n",
        "# L·∫≠p lu·∫≠n: D√πng output c·ªßa Ensemble (WA, ETS, ARIMA, LR) l√†m percentile_required\n",
        "# thay v√¨ d√πng trung b√¨nh l·ªãch s·ª≠ - v√¨ Ensemble ƒë√£ predict cho 2026!\n",
        "\n",
        "print(\"\\n[STEP 6] S·ª≠ d·ª•ng Ensemble Model 2026...\")\n",
        "\n",
        "# model_2026 ƒë√£ ƒë∆∞·ª£c train ·ªü ph·∫ßn 1 (Ensemble: WA, ETS, ARIMA, LR)\n",
        "# Format: {(university_id, ma_nganh, to_hop_mon): predicted_percentile_2026}\n",
        "\n",
        "if 'model_2026' in dir() and model_2026:\n",
        "    print(\"   ‚úÖ S·ª≠ d·ª•ng Ensemble output (model_2026)\")\n",
        "\n",
        "    # T·∫°o DataFrame t·ª´ model_2026\n",
        "    ensemble_percentile = pd.DataFrame([\n",
        "        {'university_id': k[0], 'ma_nganh': k[1], 'to_hop_mon': k[2], 'predicted_percentile_2026': v}\n",
        "        for k, v in model_2026.items()\n",
        "    ])\n",
        "    print(f\"   -> {len(ensemble_percentile)} d·ª± ƒëo√°n percentile 2026 t·ª´ Ensemble\")\n",
        "\n",
        "    # Merge v·ªõi th√¥ng tin ƒëi·ªÉm chu·∫©n ƒë·ªÉ c√≥ avg_diem_chuan\n",
        "    avg_diem = df_benchmark_v3.groupby(['university_id', 'ma_nganh', 'to_hop_mon']).agg({\n",
        "        'diem_chuan': 'mean'\n",
        "    }).reset_index()\n",
        "    avg_diem.columns = ['university_id', 'ma_nganh', 'to_hop_mon', 'avg_diem_chuan']\n",
        "\n",
        "    avg_percentile = ensemble_percentile.merge(avg_diem, on=['university_id', 'ma_nganh', 'to_hop_mon'], how='left')\n",
        "    avg_percentile.columns = ['university_id', 'ma_nganh', 'to_hop_mon', 'avg_percentile_required', 'avg_diem_chuan']\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y model_2026, s·ª≠ d·ª•ng trung b√¨nh l·ªãch s·ª≠\")\n",
        "    avg_percentile = df_benchmark_v3.groupby(['university_id', 'ma_nganh', 'to_hop_mon']).agg({\n",
        "        'percentile_required': 'mean',\n",
        "        'diem_chuan': 'mean'\n",
        "    }).reset_index()\n",
        "    avg_percentile.columns = ['university_id', 'ma_nganh', 'to_hop_mon', 'avg_percentile_required', 'avg_diem_chuan']\n",
        "\n",
        "print(f\"   -> {len(avg_percentile)} ng√†nh\")\n",
        "\n",
        "# Hi·ªÉn th·ªã m·∫´u\n",
        "print(\"\\nüìã Top 10 ng√†nh kh√≥ nh·∫•t (y√™u c·∫ßu percentile cao nh·∫•t):\")\n",
        "top_hard = avg_percentile.nsmallest(10, 'avg_percentile_required').copy()\n",
        "\n",
        "# Merge ƒë·ªÉ l·∫•y t√™n\n",
        "top_hard = top_hard.merge(\n",
        "    df_benchmark_v3[['university_id', 'ma_nganh', 'ten_truong', 'ten_nganh']].drop_duplicates(),\n",
        "    on=['university_id', 'ma_nganh']\n",
        ")\n",
        "print(top_hard[['ten_truong', 'ten_nganh', 'to_hop_mon', 'avg_diem_chuan', 'avg_percentile_required']].to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "885ad9fc",
      "metadata": {
        "id": "885ad9fc"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# B∆Ø·ªöC 7: L∆ØU T·∫§T C·∫¢ MODELS\n",
        "# =====================================================\n",
        "\n",
        "print(\"\\n[STEP 7] L∆∞u models...\")\n",
        "\n",
        "# Save classification model\n",
        "joblib.dump(gb_model, os.path.join(OUTPUT_DIR, 'admission_probability_model.pkl'))\n",
        "print(f\"   ‚úÖ Saved: admission_probability_model.pkl\")\n",
        "\n",
        "# Save encoders\n",
        "encoders = {\n",
        "    'university': le_university,\n",
        "    'nganh': le_nganh,\n",
        "    'block': le_block,\n",
        "    'feature_cols': feature_cols\n",
        "}\n",
        "joblib.dump(encoders, os.path.join(OUTPUT_DIR, 'admission_encoders.pkl'))\n",
        "print(f\"   ‚úÖ Saved: admission_encoders.pkl\")\n",
        "\n",
        "# Save scaler\n",
        "joblib.dump(scaler, os.path.join(OUTPUT_DIR, 'admission_scaler.pkl'))\n",
        "print(f\"   ‚úÖ Saved: admission_scaler.pkl\")\n",
        "\n",
        "# Save avg_percentile lookup\n",
        "joblib.dump(avg_percentile, os.path.join(OUTPUT_DIR, 'major_percentile_lookup.pkl'))\n",
        "print(f\"   ‚úÖ Saved: major_percentile_lookup.pkl\")\n",
        "\n",
        "# Save score data 2025 ƒë·ªÉ d√πng cho web app\n",
        "score_data_2025 = {k: v for k, v in all_score_data.items() if k[0] == 2025}\n",
        "joblib.dump(score_data_2025, os.path.join(OUTPUT_DIR, 'score_data_2025.pkl'))\n",
        "print(f\"   ‚úÖ Saved: score_data_2025.pkl\")\n",
        "\n",
        "print(\"\\nüéâ ƒê√£ l∆∞u t·∫•t c·∫£ models!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed07a777",
      "metadata": {
        "id": "ed07a777"
      },
      "source": [
        "## üéØ DEMO: D·ª± ƒëo√°n X√°c su·∫•t ƒê·∫≠u 2026"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59c6ee13",
      "metadata": {
        "id": "59c6ee13"
      },
      "outputs": [],
      "source": [
        "def predict_admission_probability(diem, block, university_id, ma_nganh):\n",
        "    \"\"\"\n",
        "    D·ª± ƒëo√°n x√°c su·∫•t ƒë·∫≠u cho th√≠ sinh\n",
        "\n",
        "    Args:\n",
        "        diem: ƒêi·ªÉm t·ªï h·ª£p c·ªßa th√≠ sinh\n",
        "        block: T·ªï h·ª£p m√¥n (VD: 'B00')\n",
        "        university_id: ID tr∆∞·ªùng\n",
        "        ma_nganh: M√£ ng√†nh\n",
        "\n",
        "    Returns:\n",
        "        prob: X√°c su·∫•t ƒë·∫≠u (0-1)\n",
        "        student_percentile: V·ªã tr√≠ c·ªßa th√≠ sinh\n",
        "        percentile_required: Ng√†nh y√™u c·∫ßu\n",
        "    \"\"\"\n",
        "    # 1. T√≠nh student_percentile t·ª´ ƒëi·ªÉm 2025\n",
        "    key = (2025, block)\n",
        "    if key not in all_score_data:\n",
        "        return None, None, None\n",
        "\n",
        "    scores = all_score_data[key]\n",
        "    student_percentile = (scores <= diem).sum() / len(scores) * 100\n",
        "\n",
        "    # 2. L·∫•y percentile_required t·ª´ lookup\n",
        "    lookup_row = avg_percentile[\n",
        "        (avg_percentile['university_id'] == university_id) &\n",
        "        (avg_percentile['ma_nganh'] == ma_nganh) &\n",
        "        (avg_percentile['to_hop_mon'] == block)\n",
        "    ]\n",
        "\n",
        "    if lookup_row.empty:\n",
        "        return None, student_percentile, None\n",
        "\n",
        "    percentile_required = lookup_row['avg_percentile_required'].values[0]\n",
        "\n",
        "    # 3. Encode features\n",
        "    try:\n",
        "        uni_enc = le_university.transform([str(university_id)])[0] if str(university_id) in le_university.classes_ else 0\n",
        "        nganh_enc = le_nganh.transform([str(ma_nganh)])[0] if str(ma_nganh) in le_nganh.classes_ else 0\n",
        "        block_enc = le_block.transform([block])[0] if block in le_block.classes_ else 0\n",
        "    except:\n",
        "        uni_enc, nganh_enc, block_enc = 0, 0, 0\n",
        "\n",
        "    # 4. Predict\n",
        "    features = np.array([[student_percentile, percentile_required, uni_enc, nganh_enc, block_enc]])\n",
        "    features_scaled = scaler.transform(features)\n",
        "    prob = gb_model.predict_proba(features_scaled)[0][1]\n",
        "\n",
        "    return prob, student_percentile, percentile_required\n",
        "\n",
        "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a h√†m predict_admission_probability()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78648959",
      "metadata": {
        "id": "78648959"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# DEMO: Th·ª≠ nghi·ªám d·ª± ƒëo√°n\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üéØ DEMO: D·ª∞ ƒêO√ÅN X√ÅC SU·∫§T ƒê·∫¨U 2026\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# L·∫•y danh s√°ch m·ªôt s·ªë ng√†nh ƒë·ªÉ test\n",
        "sample_majors = df_benchmark_v3[['university_id', 'ma_nganh', 'ten_truong', 'ten_nganh', 'to_hop_mon']].drop_duplicates().head(5)\n",
        "print(\"\\nüìã M·ªôt s·ªë ng√†nh ƒë·ªÉ test:\")\n",
        "print(sample_majors.to_string(index=False))\n",
        "\n",
        "# Test cases\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"üìä K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN:\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "test_cases = [\n",
        "    (27.0, 'B00', 215, '7720101', 'ƒêi·ªÉm cao (27.0)'),\n",
        "    (25.0, 'B00', 215, '7720101', 'ƒêi·ªÉm trung b√¨nh (25.0)'),\n",
        "    (23.0, 'B00', 215, '7720101', 'ƒêi·ªÉm th·∫•p (23.0)'),\n",
        "]\n",
        "\n",
        "for diem, block, uni_id, ma_nganh, desc in test_cases:\n",
        "    prob, student_pct, required_pct = predict_admission_probability(diem, block, uni_id, ma_nganh)\n",
        "\n",
        "    if prob is not None:\n",
        "        # L·∫•y t√™n ng√†nh\n",
        "        info = df_benchmark_v3[(df_benchmark_v3['university_id']==uni_id) & (df_benchmark_v3['ma_nganh']==ma_nganh)]\n",
        "        ten_nganh = info['ten_nganh'].iloc[0] if not info.empty else \"Unknown\"\n",
        "\n",
        "        print(f\"\\nüéì {ten_nganh} - {desc}\")\n",
        "        print(f\"   ƒêi·ªÉm: {diem} | T·ªï h·ª£p: {block}\")\n",
        "        print(f\"   B·∫°n thu·ªôc Top: {100-student_pct:.2f}%\")\n",
        "        print(f\"   Ng√†nh y√™u c·∫ßu Top: {100-required_pct:.2f}%\")\n",
        "        print(f\"   >>> X√ÅC SU·∫§T ƒê·∫¨U: {prob*100:.1f}%\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå {desc}: Kh√¥ng th·ªÉ d·ª± ƒëo√°n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f97d18b",
      "metadata": {
        "id": "7f97d18b"
      },
      "source": [
        "---\n",
        "## üéâ HO√ÄN TH√ÄNH!\n",
        "\n",
        "### üìä T√≥m t·∫Øt 2 Ph·∫ßn:\n",
        "\n",
        "| Ph·∫ßn | M·ª•c ti√™u | Models | Output |\n",
        "|------|----------|--------|--------|\n",
        "| **Ph·∫ßn 1** | D·ª± ƒëo√°n Percentile 2026 | WA, ETS, ARIMA, LR (Ensemble) | \"Y khoa s·∫Ω l·∫•y Top 0.5%\" |\n",
        "| **Ph·∫ßn 2** | D·ª± ƒëo√°n X√°c su·∫•t ƒê·∫≠u | Gradient Boosting Classifier | \"V·ªõi 27 ƒëi·ªÉm, b·∫°n c√≥ 65% ƒë·∫≠u\" |\n",
        "\n",
        "### üí° Key Takeaway - T·∫°i sao v3 ƒë√∫ng?\n",
        "\n",
        "**v1/v2 sai** v√¨ d√πng `gap = ƒëi·ªÉm - ƒëi·ªÉm_chu·∫©n`:\n",
        "- Khi train: bi·∫øt ƒëi·ªÉm chu·∫©n ‚Üí t√≠nh ƒë∆∞·ª£c gap\n",
        "- Khi predict 2026: KH√îNG bi·∫øt ƒëi·ªÉm chu·∫©n ‚Üí KH√îNG t√≠nh ƒë∆∞·ª£c gap!\n",
        "\n",
        "**v3 ƒë√∫ng** v√¨ ch·ªâ d√πng `percentile`:\n",
        "- Train: h·ªçc pattern \"ng√†nh X th∆∞·ªùng l·∫•y Top Y%\"  \n",
        "- Predict: ch·ªâ c·∫ßn bi·∫øt \"b·∫°n ƒëang Top bao nhi√™u %\" ‚Üí so s√°nh v·ªõi pattern ‚Üí ra x√°c su·∫•t\n",
        "\n",
        "### üìÅ Files Output:\n",
        "- `university_ranking_model_2026.pkl` - Percentile predictions\n",
        "- `admission_probability_model.pkl` - Probability classifier\n",
        "- `admission_encoders.pkl` - Label encoders\n",
        "- `admission_scaler.pkl` - StandardScaler\n",
        "- `major_percentile_lookup.pkl` - Lookup table\n",
        "- `score_distribution_2025.pkl` - Ph√¢n ph·ªëi ƒëi·ªÉm\n",
        "- `model_analytics.pkl` - Analytics ƒë·∫ßy ƒë·ªß\n",
        "\n",
        "---\n",
        "**¬© 2025 Advanced University Admission Predictor V2.0**"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}